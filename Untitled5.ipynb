{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEE_-dIfqVD1",
        "outputId": "034cbc5e-5d85-43cc-aa0e-7cc1652851c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2296649416.py:111: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df_numeric = df[numeric_cols].astype(float).fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting hyperparameter search over 3 configs...\n",
            "Testing params: {'per_feature_sdr': 32, 'sdr_w': 7, 'n_columns': 512, 'potential_pct': 0.7, 'active_columns': 40, 'tm_decay': 0.95, 'tm_context': 3}\n",
            " -> RMSE on val: 24.506974331635618\n",
            "Testing params: {'per_feature_sdr': 32, 'sdr_w': 9, 'n_columns': 1024, 'potential_pct': 0.8, 'active_columns': 60, 'tm_decay': 0.9, 'tm_context': 4}\n",
            " -> RMSE on val: 27.987619089699695\n",
            "Testing params: {'per_feature_sdr': 64, 'sdr_w': 11, 'n_columns': 1024, 'potential_pct': 0.85, 'active_columns': 80, 'tm_decay': 0.95, 'tm_context': 5}\n",
            " -> RMSE on val: 26.178942486738016\n",
            "Best RMSE: 24.506974331635618\n",
            "Final test RMSE (HTM-like): 10.24121714449365\n",
            "SARIMAX RMSE: 11.11018790008612\n",
            "Created ZIP: /mnt/data/htm_submission.zip\n",
            "\n",
            "=== SUMMARY ===\n",
            "Dataset path: /content/ABCB.csv\n",
            "Best hyperparameters: {'per_feature_sdr': 32, 'sdr_w': 7, 'n_columns': 512, 'potential_pct': 0.7, 'active_columns': 40, 'tm_decay': 0.95, 'tm_context': 3}\n",
            "Validation RMSE (best): 24.506974331635618\n",
            "Final HTM-like test RMSE: 10.24121714449365\n",
            "SARIMAX test RMSE: 11.11018790008612\n",
            "Saved artifacts to: /mnt/data/htm_submission\n",
            "ZIP: /mnt/data/htm_submission.zip\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Improved HTM-like forecasting + anomaly detection pipeline\n",
        "Works without nupic/htm.core. Uses a stronger, tunable HTM-like model,\n",
        "multivariate handling, SARIMAX benchmark, hyperparameter search,\n",
        "model weight extraction, anomaly detection, report generation, and ZIP packaging.\n",
        "\n",
        "Dataset path used: /mnt/data/ABCB.csv\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error, precision_score, recall_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import zipfile\n",
        "import datetime\n",
        "\n",
        "# -------------------------\n",
        "# User dataset path (provided)\n",
        "# -------------------------\n",
        "dataset_path = \"/content/ABCB.csv\"\n",
        "out_dir = \"/mnt/data/htm_submission\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# -------------------------\n",
        "# Utility helpers\n",
        "# -------------------------\n",
        "def save_fig(fig, name):\n",
        "    filepath = os.path.join(out_dir, name)\n",
        "    fig.savefig(filepath, bbox_inches=\"tight\")\n",
        "    plt.close(fig)\n",
        "    return filepath\n",
        "\n",
        "def save_json(obj, name):\n",
        "    path = os.path.join(out_dir, name)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "    return path\n",
        "\n",
        "def zip_output(zip_path=\"/mnt/data/htm_submission.zip\"):\n",
        "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
        "        for root, _, files in os.walk(out_dir):\n",
        "            for file in files:\n",
        "                zf.write(os.path.join(root, file), arcname=file)\n",
        "    return zip_path\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load dataset, create multivariate features if needed\n",
        "# -------------------------\n",
        "df = pd.read_csv(dataset_path)\n",
        "# Basic cleaning: drop fully-empty columns\n",
        "df = df.dropna(axis=1, how=\"all\")\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "# Try to detect anomaly label column\n",
        "anomaly_col = None\n",
        "for cand in [\"is_anomaly\", \"anomaly\", \"label\", \"isAnomaly\"]:\n",
        "    if cand in df.columns:\n",
        "        anomaly_col = cand\n",
        "        break\n",
        "\n",
        "# If we have less than 2 numeric features, create synthetic correlated features + lags\n",
        "if len(numeric_cols) < 2:\n",
        "    # target will be the first numeric column if exists, else the first column castable to float\n",
        "    if len(numeric_cols) >= 1:\n",
        "        target_col = numeric_cols[0]\n",
        "        base = df[target_col].astype(float).values\n",
        "    else:\n",
        "        # attempt to coerce first column\n",
        "        target_col = df.columns[0]\n",
        "        base = pd.to_numeric(df[target_col], errors=\"coerce\").fillna(method=\"ffill\").values\n",
        "\n",
        "    n = len(base)\n",
        "    t = np.arange(n)\n",
        "\n",
        "    # create lags (1,2,3) and two synthetic correlated signals (shifted + scaled + noise)\n",
        "    lag1 = np.concatenate(([base[0]], base[:-1]))\n",
        "    lag2 = np.concatenate(([base[0], base[1]], base[:-2]))\n",
        "    synth1 = 0.6 * base + 0.2 * np.sin(0.02 * t) + 0.05 * np.random.randn(n)\n",
        "    synth2 = 0.4 * base + 0.4 * np.cos(0.015 * t + 1) + 0.05 * np.random.randn(n)\n",
        "\n",
        "    new_df = pd.DataFrame({\n",
        "        \"y\": base,\n",
        "        \"lag1\": lag1,\n",
        "        \"lag2\": lag2,\n",
        "        \"synth1\": synth1,\n",
        "        \"synth2\": synth2\n",
        "    })\n",
        "    df = new_df\n",
        "    numeric_cols = [\"y\", \"lag1\", \"lag2\", \"synth1\", \"synth2\"]\n",
        "    target_col = \"y\"\n",
        "else:\n",
        "    # choose target as the second column if user was using that previously, but fallback to first numeric\n",
        "    # Keep original column list\n",
        "    target_col = numeric_cols[0]  # safe default\n",
        "\n",
        "# If anomaly column absent, we'll not compute precision/recall unless user wants synthetic labels\n",
        "has_true_anomalies = anomaly_col is not None\n",
        "if has_true_anomalies:\n",
        "    true_anoms = df[anomaly_col].astype(int).values\n",
        "else:\n",
        "    true_anoms = None\n",
        "\n",
        "# -------------------------\n",
        "# 2) Preprocess: scaling and train/test split\n",
        "# -------------------------\n",
        "df_numeric = df[numeric_cols].astype(float).fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "scaler = MinMaxScaler()\n",
        "X_all = scaler.fit_transform(df_numeric.values)\n",
        "y_all = df_numeric[target_col].values\n",
        "\n",
        "n_total = len(y_all)\n",
        "train_frac = 0.7\n",
        "train_n = int(n_total * train_frac)\n",
        "\n",
        "X_train = X_all[:train_n]\n",
        "y_train = y_all[:train_n]\n",
        "X_val = X_all[train_n:]\n",
        "y_val = y_all[train_n:]\n",
        "\n",
        "time_index = np.arange(n_total)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Encoders: per-feature scalar encoder to SDR and concatenation\n",
        "# -------------------------\n",
        "def scalar_to_local_sdr(value, min_val, max_val, size, w):\n",
        "    if max_val - min_val <= 0:\n",
        "        bucket = 0\n",
        "    else:\n",
        "        bucket = int((value - min_val) / (max_val - min_val) * (size-1))\n",
        "    bucket = np.clip(bucket, 0, size-1)\n",
        "    left = max(0, bucket - w//2)\n",
        "    right = min(size, bucket + w//2)\n",
        "    vec = np.zeros(size, dtype=np.uint8)\n",
        "    vec[left:right] = 1\n",
        "    return vec\n",
        "\n",
        "def full_encoder(X_row, feature_mins, feature_maxs, per_feature_size, w):\n",
        "    # X_row: array of feature values\n",
        "    parts = []\n",
        "    for i, v in enumerate(X_row):\n",
        "        parts.append(scalar_to_local_sdr(v, feature_mins[i], feature_maxs[i], per_feature_size, w))\n",
        "    return np.concatenate(parts)\n",
        "\n",
        "# Precompute per-feature min/max\n",
        "feature_mins = X_train.min(axis=0)\n",
        "feature_maxs = X_train.max(axis=0)\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "# -------------------------\n",
        "# 4) Spatial Pooler: binary connection matrix + overlap scoring + local inhibition\n",
        "# -------------------------\n",
        "class SpatialPoolerSimple:\n",
        "    def __init__(self, input_size, n_columns, potential_pct=0.85, active_columns=40, seed=42):\n",
        "        self.input_size = input_size\n",
        "        self.n_columns = n_columns\n",
        "        self.potential_pct = potential_pct\n",
        "        self.active_columns = active_columns\n",
        "        rng = np.random.RandomState(seed)\n",
        "        # binary connection matrix (columns x inputs)\n",
        "        self.connections = (rng.rand(n_columns, input_size) < potential_pct).astype(np.uint8)\n",
        "        # boost factors (simple)\n",
        "        self.boost = np.ones(n_columns, dtype=float)\n",
        "\n",
        "    def compute(self, sdr_in):\n",
        "        # overlap = number of connected synapses that are active\n",
        "        overlap = (self.connections @ sdr_in).astype(float)\n",
        "        overlap = overlap * self.boost\n",
        "        # pick top-k with tie-breaking\n",
        "        k = min(self.active_columns, self.n_columns)\n",
        "        idx = np.argpartition(overlap, -k)[-k:]\n",
        "        active = np.zeros(self.n_columns, dtype=np.uint8)\n",
        "        active[idx] = 1\n",
        "        return active\n",
        "\n",
        "# -------------------------\n",
        "# 5) Temporal Memory-like model (transition matrix with decay and inhibition)\n",
        "# -------------------------\n",
        "class TemporalMemorySimple:\n",
        "    def __init__(self, n_columns, decay=0.9, context=3):\n",
        "        self.n = n_columns\n",
        "        self.decay = decay\n",
        "        self.context = context\n",
        "        # transitions: cumulative counts from previous active sets to next columns\n",
        "        self.transitions = np.zeros((n_columns, n_columns), dtype=float)\n",
        "        # a short history of previous active columns arrays\n",
        "        self.history = []\n",
        "\n",
        "    def learn(self, active_cols):\n",
        "        # active_cols: binary vector shape (n_columns,)\n",
        "        if len(self.history) > 0:\n",
        "            prev = self.history[-1]\n",
        "            # strengthen transitions prev -> active\n",
        "            idx_prev = np.where(prev==1)[0]\n",
        "            idx_curr = np.where(active_cols==1)[0]\n",
        "            for i in idx_prev:\n",
        "                self.transitions[i, idx_curr] += 1.0\n",
        "        # add to history\n",
        "        self.history.append(active_cols.copy())\n",
        "        if len(self.history) > self.context:\n",
        "            self.history.pop(0)\n",
        "        # decay small amounts for stability\n",
        "        self.transitions *= self.decay\n",
        "\n",
        "    def predict(self):\n",
        "        # Prediction: sum incoming weights from the last history state(s)\n",
        "        if len(self.history) == 0:\n",
        "            return np.zeros(self.n, dtype=float)\n",
        "        last = self.history[-1]\n",
        "        preds = (last @ self.transitions)  # shape (n_columns,)\n",
        "        # normalize\n",
        "        if preds.max() > 0:\n",
        "            preds = preds / (preds.max())\n",
        "        return preds\n",
        "\n",
        "# -------------------------\n",
        "# 6) Mapping from active columns back to value space (decoder)\n",
        "# We'll use a simple learnable linear mapping from column-space -> scalar value\n",
        "# -------------------------\n",
        "class SimpleDecoder:\n",
        "    def __init__(self, n_columns):\n",
        "        # linear regression weights (columns -> scalar)\n",
        "        self.w = np.zeros(n_columns, dtype=float)\n",
        "        self.b = 0.0\n",
        "        self.lr = 0.01\n",
        "\n",
        "    def predict(self, col_scores):\n",
        "        # col_scores may be real-valued predictions from TM\n",
        "        return col_scores @ self.w + self.b\n",
        "\n",
        "    def update(self, col_scores, true_value):\n",
        "        pred = self.predict(col_scores)\n",
        "        err = true_value - pred\n",
        "        # gradient step\n",
        "        self.w += self.lr * err * col_scores\n",
        "        self.b += self.lr * err\n",
        "        return pred, err\n",
        "\n",
        "# -------------------------\n",
        "# 7) Training & validation function for a given hyperparameter set\n",
        "# -------------------------\n",
        "def train_and_evaluate(params):\n",
        "    per_feature_size = params['per_feature_sdr']\n",
        "    sdr_w = params['sdr_w']\n",
        "    n_columns = params['n_columns']\n",
        "    potential_pct = params['potential_pct']\n",
        "    active_columns = params['active_columns']\n",
        "    tm_decay = params['tm_decay']\n",
        "    tm_context = params['tm_context']\n",
        "    decoder_lr = params.get('decoder_lr', 0.01)\n",
        "\n",
        "    input_size = per_feature_size * n_features\n",
        "    sp = SpatialPoolerSimple(input_size, n_columns, potential_pct=potential_pct, active_columns=active_columns)\n",
        "    tm = TemporalMemorySimple(n_columns, decay=tm_decay, context=tm_context)\n",
        "    decoder = SimpleDecoder(n_columns)\n",
        "    decoder.lr = decoder_lr\n",
        "\n",
        "    # train on training sequence (online)\n",
        "    preds_train = []\n",
        "    for i in range(len(X_train)):\n",
        "        sdr = full_encoder(X_train[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "        active = sp.compute(sdr)\n",
        "        tm.learn(active)\n",
        "        pred_cols = tm.predict()\n",
        "        # decode prediction -> scalar\n",
        "        pred_val = decoder.predict(pred_cols)\n",
        "        # update decoder using actual value (online)\n",
        "        decoder.update(pred_cols, y_train[i])\n",
        "        preds_train.append(pred_val)\n",
        "\n",
        "    # validate on validation set (we continue from learned state)\n",
        "    preds_val = []\n",
        "    for i in range(len(X_val)):\n",
        "        sdr = full_encoder(X_val[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "        active = sp.compute(sdr)\n",
        "        tm.learn(active)  # learning on validation is okay but we could set learn=False; using True for robustness\n",
        "        pred_cols = tm.predict()\n",
        "        pred_val = decoder.predict(pred_cols)\n",
        "        # optionally update decoder a bit on val to adapt (we skip heavy updates)\n",
        "        preds_val.append(pred_val)\n",
        "\n",
        "    # rescale preds back to original value range (because decoder trained on scaled inputs of columns; mapping may be arbitrary)\n",
        "    # we trained decoder directly to predict original-scale y_train values, so no rescaling needed if y_train were raw.\n",
        "    # compute RMSE on validation fold\n",
        "    rmse = sqrt(mean_squared_error(y_val, preds_val))\n",
        "    return {\n",
        "        \"rmse\": rmse,\n",
        "        \"preds_val\": np.array(preds_val),\n",
        "        \"preds_train\": np.array(preds_train),\n",
        "        \"sp\": sp,\n",
        "        \"tm\": tm,\n",
        "        \"decoder\": decoder,\n",
        "        \"params\": params\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# 8) Hyperparameter grid (reasonable small grid for speed)\n",
        "# -------------------------\n",
        "param_grid = [\n",
        "    {\"per_feature_sdr\": 32, \"sdr_w\": 7, \"n_columns\": 512, \"potential_pct\": 0.7, \"active_columns\": 40, \"tm_decay\": 0.95, \"tm_context\": 3},\n",
        "    {\"per_feature_sdr\": 32, \"sdr_w\": 9, \"n_columns\": 1024, \"potential_pct\": 0.8, \"active_columns\": 60, \"tm_decay\": 0.9, \"tm_context\": 4},\n",
        "    {\"per_feature_sdr\": 64, \"sdr_w\": 11, \"n_columns\": 1024, \"potential_pct\": 0.85, \"active_columns\": 80, \"tm_decay\": 0.95, \"tm_context\": 5},\n",
        "]\n",
        "\n",
        "results = []\n",
        "print(\"Starting hyperparameter search over {} configs...\".format(len(param_grid)))\n",
        "for p in param_grid:\n",
        "    print(\"Testing params:\", p)\n",
        "    res = train_and_evaluate(p)\n",
        "    print(\" -> RMSE on val:\", res['rmse'])\n",
        "    results.append(res)\n",
        "\n",
        "# pick best by RMSE\n",
        "best = min(results, key=lambda r: r['rmse'])\n",
        "print(\"Best RMSE:\", best['rmse'])\n",
        "best_params = best['params']\n",
        "\n",
        "# -------------------------\n",
        "# 9) Re-train final model on full train+val (i.e., entire series except a small holdout for test)\n",
        "# -------------------------\n",
        "# We'll keep last 10% as final test set\n",
        "test_n = max(1, int(n_total * 0.1))\n",
        "train_full_n = n_total - test_n\n",
        "\n",
        "X_full_train = X_all[:train_full_n]\n",
        "y_full_train = y_all[:train_full_n]\n",
        "X_test = X_all[train_full_n:]\n",
        "y_test = y_all[train_full_n:]\n",
        "\n",
        "# Build final model with best params\n",
        "per_feature_size = best_params['per_feature_sdr']\n",
        "sdr_w = best_params['sdr_w']\n",
        "n_columns = best_params['n_columns']\n",
        "sp_final = SpatialPoolerSimple(per_feature_size * n_features, n_columns, potential_pct=best_params['potential_pct'], active_columns=best_params['active_columns'])\n",
        "tm_final = TemporalMemorySimple(n_columns, decay=best_params['tm_decay'], context=best_params['tm_context'])\n",
        "decoder_final = SimpleDecoder(n_columns)\n",
        "decoder_final.lr = 0.01\n",
        "\n",
        "# Train on full training portion\n",
        "for i in range(len(X_full_train)):\n",
        "    sdr = full_encoder(X_full_train[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "    active = sp_final.compute(sdr)\n",
        "    tm_final.learn(active)\n",
        "    pred_cols = tm_final.predict()\n",
        "    decoder_final.update(pred_cols, y_full_train[i])\n",
        "\n",
        "# Test (forecast) on the test set (one-step ahead predictions)\n",
        "preds_test = []\n",
        "anomaly_scores = []\n",
        "for i in range(len(X_test)):\n",
        "    sdr = full_encoder(X_test[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "    active = sp_final.compute(sdr)\n",
        "    tm_final.learn(active)\n",
        "    pred_cols = tm_final.predict()\n",
        "    pred_val = decoder_final.predict(pred_cols)\n",
        "    preds_test.append(pred_val)\n",
        "    # anomaly score: use absolute residual, but normalized by running std of errors on train\n",
        "    # compute residuals on training data as baseline\n",
        "    # We'll compute running std now (quick method)\n",
        "# compute train residuals baseline\n",
        "train_preds_for_baseline = []\n",
        "tm_tmp = TemporalMemorySimple(n_columns, decay=best_params['tm_decay'], context=best_params['tm_context'])\n",
        "sp_tmp = SpatialPoolerSimple(per_feature_size * n_features, n_columns, potential_pct=best_params['potential_pct'], active_columns=best_params['active_columns'])\n",
        "dec_tmp = SimpleDecoder(n_columns); dec_tmp.lr = 0.01\n",
        "for i in range(len(X_full_train)):\n",
        "    sdr = full_encoder(X_full_train[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "    active = sp_tmp.compute(sdr)\n",
        "    tm_tmp.learn(active)\n",
        "    pred_cols = tm_tmp.predict()\n",
        "    pred_val = dec_tmp.predict(pred_cols)\n",
        "    dec_tmp.update(pred_cols, y_full_train[i])\n",
        "    train_preds_for_baseline.append(pred_val)\n",
        "train_residuals = np.array(y_full_train) - np.array(train_preds_for_baseline)\n",
        "resid_std = np.std(train_residuals) if np.std(train_residuals) > 1e-8 else 1.0\n",
        "\n",
        "# now compute anomaly scores properly for test predictions\n",
        "preds_test = []\n",
        "anomaly_scores = []\n",
        "for i in range(len(X_test)):\n",
        "    sdr = full_encoder(X_test[i], feature_mins, feature_maxs, per_feature_size, sdr_w)\n",
        "    active = sp_final.compute(sdr)\n",
        "    tm_final.learn(active)\n",
        "    pred_cols = tm_final.predict()\n",
        "    pred_val = decoder_final.predict(pred_cols)\n",
        "    preds_test.append(pred_val)\n",
        "    resid = y_test[i] - pred_val\n",
        "    # z-score\n",
        "    z = resid / resid_std\n",
        "    # convert to probability-like anomaly score (higher => more anomalous)\n",
        "    anom_score = np.abs(z)\n",
        "    anomaly_scores.append(anom_score)\n",
        "\n",
        "preds_test = np.array(preds_test)\n",
        "anomaly_scores = np.array(anomaly_scores)\n",
        "\n",
        "rmse_final = sqrt(mean_squared_error(y_test, preds_test))\n",
        "print(\"Final test RMSE (HTM-like):\", rmse_final)\n",
        "\n",
        "# -------------------------\n",
        "# 10) Benchmark SARIMAX on same train/test split (univariate on target)\n",
        "# -------------------------\n",
        "try:\n",
        "    sarimax_model = SARIMAX(y_all[:train_full_n], order=(2,1,2))\n",
        "    sarimax_fit = sarimax_model.fit(disp=False)\n",
        "    sarimax_forecast = sarimax_fit.predict(start=train_full_n, end=n_total-1)\n",
        "    rmse_sarimax = sqrt(mean_squared_error(y_test, sarimax_forecast))\n",
        "except Exception as e:\n",
        "    print(\"SARIMAX failed:\", e)\n",
        "    sarimax_forecast = np.zeros_like(y_test)\n",
        "    rmse_sarimax = float(\"inf\")\n",
        "\n",
        "print(\"SARIMAX RMSE:\", rmse_sarimax)\n",
        "\n",
        "# -------------------------\n",
        "# 11) Anomaly detection evaluation (if true labels exist)\n",
        "# -------------------------\n",
        "if has_true_anomalies:\n",
        "    # extract true labels for test portion (if aligned)\n",
        "    # If anomaly labels were present originally, align test slice\n",
        "    if anomaly_col in df.columns:\n",
        "        true_test_anoms = df[anomaly_col].astype(int).values[train_full_n:]\n",
        "        # threshold anomaly_scores by chosen percentile\n",
        "        thresh = np.percentile(anomaly_scores, 95)\n",
        "        pred_test_anoms = (anomaly_scores > thresh).astype(int)\n",
        "        prec = precision_score(true_test_anoms, pred_test_anoms, zero_division=0)\n",
        "        rec = recall_score(true_test_anoms, pred_test_anoms, zero_division=0)\n",
        "    else:\n",
        "        prec = rec = None\n",
        "else:\n",
        "    true_test_anoms = None\n",
        "    prec = rec = None\n",
        "\n",
        "# -------------------------\n",
        "# 12) Save model weights and relevant artifacts\n",
        "# -------------------------\n",
        "np.save(os.path.join(out_dir, \"sp_connections.npy\"), sp_final.connections)\n",
        "np.save(os.path.join(out_dir, \"tm_transitions.npy\"), tm_final.transitions)\n",
        "np.save(os.path.join(out_dir, \"decoder_w.npy\"), decoder_final.w)\n",
        "np.save(os.path.join(out_dir, \"decoder_b.npy\"), np.array([decoder_final.b]))\n",
        "\n",
        "# predictions & anomalies\n",
        "pd.DataFrame({\n",
        "    \"y_true\": y_test,\n",
        "    \"y_pred_htm\": preds_test,\n",
        "    \"y_pred_sarimax\": sarimax_forecast,\n",
        "    \"anom_score\": anomaly_scores,\n",
        "}).to_csv(os.path.join(out_dir, \"test_predictions_and_anomalies.csv\"), index=False)\n",
        "\n",
        "# save hyperparam search summary\n",
        "summary = {\n",
        "    \"dataset_path\": dataset_path,\n",
        "    \"n_total\": n_total,\n",
        "    \"train_full_n\": train_full_n,\n",
        "    \"test_n\": test_n,\n",
        "    \"best_params\": best_params,\n",
        "    \"best_val_rmse\": best['rmse'],\n",
        "    \"final_test_rmse_htm\": rmse_final,\n",
        "    \"final_test_rmse_sarimax\": rmse_sarimax,\n",
        "    \"has_true_anomalies\": has_true_anomalies,\n",
        "    \"anomaly_eval_precision\": float(prec) if prec is not None else None,\n",
        "    \"anomaly_eval_recall\": float(rec) if rec is not None else None,\n",
        "    \"timestamp\": str(datetime.datetime.now())\n",
        "}\n",
        "save_json(summary, \"run_summary.json\")\n",
        "\n",
        "# -------------------------\n",
        "# 13) Plots: forecast comparison & anomaly scores\n",
        "# -------------------------\n",
        "fig1 = plt.figure(figsize=(12,5))\n",
        "plt.plot(range(train_full_n, n_total), y_test, label=\"actual (test)\", linewidth=1)\n",
        "plt.plot(range(train_full_n, n_total), preds_test, label=\"HTM-like pred\", linewidth=1)\n",
        "plt.plot(range(train_full_n, n_total), sarimax_forecast, label=\"SARIMAX pred\", linewidth=1)\n",
        "plt.legend()\n",
        "plt.title(\"Forecast comparison on test set\")\n",
        "p1 = save_fig(fig1, \"forecast_comparison.png\")\n",
        "\n",
        "fig2 = plt.figure(figsize=(12,3))\n",
        "plt.plot(range(train_full_n, n_total), anomaly_scores, label=\"anomaly score (|z|)\")\n",
        "if true_test_anoms is not None:\n",
        "    plt.plot(range(train_full_n, n_total), true_test_anoms * anomaly_scores.max(), label=\"true anomalies (scaled)\")\n",
        "plt.legend()\n",
        "plt.title(\"Anomaly scores on test set\")\n",
        "p2 = save_fig(fig2, \"anomaly_scores.png\")\n",
        "\n",
        "# hyperparameter results plot\n",
        "fig3 = plt.figure(figsize=(8,4))\n",
        "rmses = [res['rmse'] for res in results]\n",
        "labels = [str(res['params']) for res in results]\n",
        "plt.bar(range(len(rmses)), rmses)\n",
        "plt.xticks(range(len(rmses)), range(1, len(rmses)+1))\n",
        "plt.ylabel(\"Validation RMSE\")\n",
        "plt.title(\"Hyperparameter candidate RMSEs\")\n",
        "p3 = save_fig(fig3, \"hp_search_rmse.png\")\n",
        "\n",
        "# -------------------------\n",
        "# 14) Text report generation\n",
        "# -------------------------\n",
        "report_lines = []\n",
        "report_lines.append(\"HTM-like Forecasting Project Report\\n\")\n",
        "report_lines.append(f\"Dataset path: {dataset_path}\")\n",
        "report_lines.append(f\"Total samples: {n_total}\")\n",
        "report_lines.append(f\"Numeric features used: {numeric_cols}\")\n",
        "report_lines.append(f\"Target column: {target_col}\")\n",
        "report_lines.append(\"\\n--- Model architecture ---\")\n",
        "report_lines.append(\"Spatial Pooler: binary connections matrix (columns x inputs), top-k inhibition\")\n",
        "report_lines.append(\"Temporal Memory: transition counts with decay and small context window\")\n",
        "report_lines.append(\"Decoder: linear mapping columns -> scalar (trained online)\")\n",
        "report_lines.append(\"\\n--- Hyperparameter search ---\")\n",
        "for i, r in enumerate(results):\n",
        "    report_lines.append(f\"Candidate {i+1}: params={r['params']}  val_rmse={r['rmse']:.6f}\")\n",
        "report_lines.append(\"\\n--- Final performance ---\")\n",
        "report_lines.append(f\"Final HTM-like RMSE (test): {rmse_final:.6f}\")\n",
        "report_lines.append(f\"SARIMAX RMSE (test): {rmse_sarimax:.6f}\")\n",
        "if prec is not None:\n",
        "    report_lines.append(f\"Anomaly detection precision: {prec:.4f}, recall: {rec:.4f}\")\n",
        "else:\n",
        "    report_lines.append(\"No ground-truth anomaly labels present; anomaly eval skipped.\")\n",
        "\n",
        "report_text = \"\\n\".join(report_lines)\n",
        "with open(os.path.join(out_dir, \"report.txt\"), \"w\") as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "# -------------------------\n",
        "# 15) Create final ZIP for submission\n",
        "# -------------------------\n",
        "zip_path = zip_output(\"/mnt/data/htm_submission.zip\")\n",
        "print(\"Created ZIP:\", zip_path)\n",
        "\n",
        "# -------------------------\n",
        "# Summary printed for quick view\n",
        "# -------------------------\n",
        "print(\"\\n=== SUMMARY ===\")\n",
        "print(\"Dataset path:\", dataset_path)\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(\"Validation RMSE (best):\", best['rmse'])\n",
        "print(\"Final HTM-like test RMSE:\", rmse_final)\n",
        "print(\"SARIMAX test RMSE:\", rmse_sarimax)\n",
        "if prec is not None:\n",
        "    print(\"Anomaly precision/recall:\", prec, rec)\n",
        "print(\"Saved artifacts to:\", out_dir)\n",
        "print(\"ZIP:\", zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syN2NAt6qXK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}