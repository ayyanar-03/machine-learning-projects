{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LJsfJ4Le9rYt",
        "outputId": "5e679afd-7e24-4e71-acf7-b6aab7a5cddd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         f1        f2        f3        f4        f5\n",
              "0  0.024836 -0.220055  0.027987  1.038918 -0.007957\n",
              "1  0.043003  0.082118  0.019093  0.971191  0.034539\n",
              "2  0.131719  0.001046  0.002393  0.954094  0.086051\n",
              "3  0.223912  0.148608 -0.011139  0.988602  0.205961\n",
              "4  0.183002  0.204392  0.016364  0.971557  0.083116"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11d5dfe9-633f-438a-b285-69f68be05161\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.024836</td>\n",
              "      <td>-0.220055</td>\n",
              "      <td>0.027987</td>\n",
              "      <td>1.038918</td>\n",
              "      <td>-0.007957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.043003</td>\n",
              "      <td>0.082118</td>\n",
              "      <td>0.019093</td>\n",
              "      <td>0.971191</td>\n",
              "      <td>0.034539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.131719</td>\n",
              "      <td>0.001046</td>\n",
              "      <td>0.002393</td>\n",
              "      <td>0.954094</td>\n",
              "      <td>0.086051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.223912</td>\n",
              "      <td>0.148608</td>\n",
              "      <td>-0.011139</td>\n",
              "      <td>0.988602</td>\n",
              "      <td>0.205961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183002</td>\n",
              "      <td>0.204392</td>\n",
              "      <td>0.016364</td>\n",
              "      <td>0.971557</td>\n",
              "      <td>0.083116</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11d5dfe9-633f-438a-b285-69f68be05161')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11d5dfe9-633f-438a-b285-69f68be05161 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11d5dfe9-633f-438a-b285-69f68be05161');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab3a5d26-1e3f-45e3-8cdc-fdab54d83497\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab3a5d26-1e3f-45e3-8cdc-fdab54d83497')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab3a5d26-1e3f-45e3-8cdc-fdab54d83497 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 500,\n  \"fields\": [\n    {\n      \"column\": \"f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3545127606882463,\n        \"min\": -0.6008951254011942,\n        \"max\": 0.6362153388789108,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          -0.42316262562627255,\n          0.5034504931049828,\n          -0.03967667787455098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35686269126280346,\n        \"min\": -0.5976012041676174,\n        \"max\": 0.6425458628949894,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          -0.5153944244594162,\n          0.388469614104813,\n          -0.14733033894562197\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08904534726015001,\n        \"min\": -0.02037085241260151,\n        \"max\": 0.34576398571336503,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          0.22818582999202175,\n          0.018140155516228607,\n          0.2538234065498722\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7111174727900268,\n        \"min\": -1.1203471632869266,\n        \"max\": 1.0902069858340948,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          0.737565616686434,\n          -0.9413499879151079,\n          1.0543527800001264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.25493647236080536,\n        \"min\": -0.5153015639840893,\n        \"max\": 0.5368795962052353,\n        \"num_unique_values\": 500,\n        \"samples\": [\n          -0.379743717220869,\n          -0.46405733022995094,\n          0.014036725176920707\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 500  # number of timesteps\n",
        "\n",
        "time = np.arange(n)\n",
        "\n",
        "# Features with temporal dependencies\n",
        "feature_1 = 0.5 * np.sin(0.1 * time) + np.random.normal(0, 0.05, n)\n",
        "feature_2 = np.roll(feature_1, 1) + np.random.normal(0, 0.03, n)  # lag relationship\n",
        "feature_3 = 0.3 * time / n + np.random.normal(0, 0.02, n)  # upward trend\n",
        "feature_4 = np.cos(0.05 * time) + np.random.normal(0, 0.05, n)\n",
        "feature_5 = feature_1 * feature_4 + np.random.normal(0, 0.05, n) # nonlinear dependency\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"f1\": feature_1,\n",
        "    \"f2\": feature_2,\n",
        "    \"f3\": feature_3,\n",
        "    \"f4\": feature_4,\n",
        "    \"f5\": feature_5,\n",
        "})\n",
        "\n",
        "df.to_csv(\"dataset.csv\", index=False)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Full HTM + SARIMAX comparison script for a multivariate time series.\n",
        "Uses the uploaded dataset at: /mnt/data/ABCB.csv\n",
        "\n",
        "Requirements (suggested):\n",
        "  pip install pandas numpy matplotlib scikit-learn statsmodels htm-core\n",
        "\n",
        "This script implements:\n",
        " - Scalar encoding of features into binary-like encodings (uses htm bindings if available)\n",
        " - Spatial Pooler + Temporal Memory (attempts to use htm.core bindings if installed)\n",
        " - A simple associative predictor based on active columns -> average next target value\n",
        " - SARIMAX baseline\n",
        " - RMSE comparison and anomaly detection metrics (precision/recall)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, precision_score, recall_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# --- Local dataset path (you uploaded this file) ---\n",
        "DATA_PATH = \"/content/ABCB.csv\"\n",
        "\n",
        "# --- User-tweakable settings ---\n",
        "TARGET_COL = None  # set to None to auto-select first numeric column; or \"f1\" etc.\n",
        "TEST_SIZE = 0.2    # fraction for test set\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# --- HTM-related imports with fallbacks ---\n",
        "try:\n",
        "    # Primary try: htm.core binding layout\n",
        "    from htm.bindings.sdr import SDR\n",
        "    try:\n",
        "        # newer bindings locations\n",
        "        from htm.bindings.algorithms import SpatialPooler as SpatialPoolerBinding\n",
        "        from htm.bindings.algorithms import TemporalMemory as TemporalMemoryBinding\n",
        "    except Exception:\n",
        "        # alternative names / modules (some installations differ)\n",
        "        from htm.bindings.algorithms import SP as SpatialPoolerBinding\n",
        "        from htm.bindings.algorithms import TM as TemporalMemoryBinding\n",
        "\n",
        "    # Try encoders & anomaly likelihood\n",
        "    try:\n",
        "        from htm.bindings.encoders import ScalarEncoder as ScalarEncoderBinding\n",
        "    except Exception:\n",
        "        # fallback location\n",
        "        from htm.bindings.encoders import ScalarEncoder as ScalarEncoderBinding\n",
        "\n",
        "    try:\n",
        "        from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
        "    except Exception:\n",
        "        # older/newer paths\n",
        "        from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
        "\n",
        "    HTM_AVAILABLE = True\n",
        "    print(\"htm.core bindings appeared available. Using native SDR/SP/TM bindings where possible.\")\n",
        "except Exception as e:\n",
        "    # If HTM bindings are not installed, we'll use lightweight-compatible fallbacks.\n",
        "    HTM_AVAILABLE = False\n",
        "    print(\"htm.core bindings NOT available (or import failed). Falling back to simplified HTM-like components.\")\n",
        "    # Minimal SDR class fallback\n",
        "    class SDR:\n",
        "        def __init__(self, size):\n",
        "            self.size = size\n",
        "            self.bits = np.zeros(size, dtype=np.int32)\n",
        "        def clear(self):\n",
        "            self.bits[:] = 0\n",
        "        def setDense(self, arr):\n",
        "            arr = np.asarray(arr)\n",
        "            if arr.size != self.size:\n",
        "                raise ValueError(\"SDR.setDense: size mismatch\")\n",
        "            self.bits = (arr != 0).astype(np.int32)\n",
        "        def dense(self):\n",
        "            return self.bits.copy()\n",
        "\n",
        "    # Define very small SpatialPooler-like component\n",
        "    class SpatialPoolerBinding:\n",
        "        def __init__(self, inputDimensions, columnDimensions, potentialPct=0.85,\n",
        "                     globalInhibition=True, numActiveColumnsPerInhArea=40):\n",
        "            self.inputDimensions = inputDimensions\n",
        "            self.columnDimensions = columnDimensions\n",
        "            self.numActiveColumns = int(numActiveColumnsPerInhArea)\n",
        "            self.columns = self.columnDimensions[0]\n",
        "        def compute(self, input_sdr, learn, activeArray):\n",
        "            # naive: pick columns by hashing active input bits\n",
        "            inp = input_sdr if isinstance(input_sdr, np.ndarray) else input_sdr.dense()\n",
        "            scores = np.dot((inp>0).astype(np.int32), np.random.RandomState(1).rand(self.columns))\n",
        "            topk_idx = np.argsort(scores)[-self.numActiveColumns:]\n",
        "            activeArray.clear()\n",
        "            activeArray[topk_idx] = 1\n",
        "\n",
        "    class TemporalMemoryBinding:\n",
        "        def __init__(self, columnDimensions, cellsPerColumn=4, activationThreshold=12,\n",
        "                     initialPermanence=0.21, connectedPermanence=0.5):\n",
        "            self.columns = columnDimensions[0]\n",
        "            self.cellsPerColumn = cellsPerColumn\n",
        "            self._active_cells = set()\n",
        "        def compute(self, activeColumns, learn=True):\n",
        "            # naive: represent active cells as column_index*cpc + 0\n",
        "            cols = np.where(activeColumns.dense() > 0)[0]\n",
        "            self._active_cells = set([c * self.cellsPerColumn for c in cols])\n",
        "        def getActiveCells(self):\n",
        "            return np.array(sorted(list(self._active_cells)))\n",
        "    # Minimal scalar encoder fallback\n",
        "    class ScalarEncoderBinding:\n",
        "        def __init__(self, n=50, w=21, minval=0.0, maxval=1.0):\n",
        "            self.n = n\n",
        "            self.w = w\n",
        "            self.minval = float(minval)\n",
        "            self.maxval = float(maxval)\n",
        "            self.buckets = np.linspace(self.minval, self.maxval, self.n - self.w + 1)\n",
        "        def encode(self, value):\n",
        "            v = float(value)\n",
        "            pos = np.searchsorted(self.buckets, v)\n",
        "            out = np.zeros(self.n, dtype=np.int32)\n",
        "            start = max(0, min(self.n - self.w, pos))\n",
        "            out[start:start + self.w] = 1\n",
        "            return out\n",
        "    # Minimal anomaly likelihood stub\n",
        "    class AnomalyLikelihood:\n",
        "        def __init__(self):\n",
        "            pass\n",
        "        def anomalyProbability(self, value, timestamp=None):\n",
        "            # naive mapping\n",
        "            return float(value)\n",
        "\n",
        "# --- Utility functions ---------------------------------------------------\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Dataset not found at: {path}\")\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "def choose_target_column(df, target=None):\n",
        "    if target and target in df.columns:\n",
        "        return target\n",
        "    # choose first numeric column\n",
        "    numeric = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not numeric:\n",
        "        raise ValueError(\"No numeric columns found in the dataset to predict.\")\n",
        "    return numeric[0]\n",
        "\n",
        "def train_test_split_series(df, test_fraction=0.2):\n",
        "    n = len(df)\n",
        "    split = int((1 - test_fraction) * n)\n",
        "    return df.iloc[:split].reset_index(drop=True), df.iloc[split:].reset_index(drop=True)\n",
        "\n",
        "# --- Load dataset -------------------------------------------------------\n",
        "df = safe_read_csv(DATA_PATH)\n",
        "print(\"Loaded dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Choose target column\n",
        "TARGET_COL = choose_target_column(df, TARGET_COL)\n",
        "print(\"Target column selected for one-step-ahead forecasting:\", TARGET_COL)\n",
        "\n",
        "# Basic preprocessing: fill missing, scale features\n",
        "df = df.copy()\n",
        "df = df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
        "\n",
        "feature_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if TARGET_COL not in feature_cols:\n",
        "    raise ValueError(\"Selected target column is not numeric.\")\n",
        "\n",
        "# Standardize features for better encoding stability (retain original target for evaluation)\n",
        "scaler = StandardScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df[feature_cols]), columns=feature_cols)\n",
        "\n",
        "# Train/test split\n",
        "train_df_scaled, test_df_scaled = train_test_split_series(df_scaled, TEST_SIZE)\n",
        "train_df_orig, test_df_orig = train_test_split_series(df[feature_cols], TEST_SIZE)  # original-scale for metrics\n",
        "\n",
        "print(\"Train shape:\", train_df_scaled.shape, \"Test shape:\", test_df_scaled.shape)\n",
        "\n",
        "# --- Build encoders for each feature -----------------------------------\n",
        "encoders = {}\n",
        "encoder_widths = {}\n",
        "for name in feature_cols:\n",
        "    vmin = float(df_scaled[name].min())\n",
        "    vmax = float(df_scaled[name].max())\n",
        "    # parameters chosen so encodings are reasonably wide\n",
        "    enc = ScalarEncoderBinding(n=100, w=21, minval=vmin, maxval=vmax)\n",
        "    encoders[name] = enc\n",
        "    encoder_widths[name] = enc.n if hasattr(enc, \"n\") else (enc.getWidth() if hasattr(enc, \"getWidth\") else 100)\n",
        "\n",
        "# Input SDR dimensionality\n",
        "try:\n",
        "    # prefer binding-provided method if present\n",
        "    input_dim = sum(enc.getWidth() for enc in encoders.values())\n",
        "except Exception:\n",
        "    # fallback: sum encoder.n\n",
        "    input_dim = sum(getattr(enc, \"n\", encoder_widths[name]) for name, enc in encoders.items())\n",
        "\n",
        "print(\"Total input SDR dimension (approx):\", input_dim)\n",
        "\n",
        "# Instantiate SDR container\n",
        "sdr = SDR(input_dim)\n",
        "\n",
        "# Spatial Pooler settings (you can tune these)\n",
        "SP_PARAMS = dict(\n",
        "    inputDimensions=(input_dim,),\n",
        "    columnDimensions=(1024,),\n",
        "    potentialPct=0.85,\n",
        "    globalInhibition=True,\n",
        "    numActiveColumnsPerInhArea=40,\n",
        ")\n",
        "\n",
        "# Temporal Memory settings\n",
        "TM_PARAMS = dict(\n",
        "    columnDimensions=(SP_PARAMS[\"columnDimensions\"][0],),\n",
        "    cellsPerColumn=16,\n",
        "    activationThreshold=12,\n",
        "    initialPermanence=0.21,\n",
        "    connectedPermanence=0.5,\n",
        ")\n",
        "\n",
        "# Instantiate SP and TM (using either bindings or fallbacks)\n",
        "sp = SpatialPoolerBinding(**SP_PARAMS)\n",
        "tm = TemporalMemoryBinding(**TM_PARAMS)\n",
        "\n",
        "# --- Associative predictor: mapping active columns -> list of next-target values ---\n",
        "assoc_predictor = {}  # key: bytes(active_cols) -> list of next-target-value(s)\n",
        "\n",
        "def encode_row_to_sdr(row_scaled):\n",
        "    \"\"\"\n",
        "    Encode a pandas Series (scaled features) into a binary vector (concatenated encodings).\n",
        "    Returns a 1D numpy array of 0/1 length = input_dim.\n",
        "    \"\"\"\n",
        "    parts = []\n",
        "    for name in feature_cols:\n",
        "        val = float(row_scaled[name])\n",
        "        enc = encoders[name]\n",
        "        arr = enc.encode(val)\n",
        "        parts.append(np.asarray(arr, dtype=np.int32))\n",
        "    out = np.concatenate(parts).astype(np.int32)\n",
        "    if out.size != input_dim:\n",
        "        # pad/truncate to match input_dim\n",
        "        if out.size < input_dim:\n",
        "            pad = np.zeros(input_dim - out.size, dtype=np.int32)\n",
        "            out = np.concatenate([out, pad])\n",
        "        else:\n",
        "            out = out[:input_dim]\n",
        "    return out\n",
        "\n",
        "def active_array_to_bytes(active_array):\n",
        "    # active_array: SDR-like object with .dense() or numpy array\n",
        "    if hasattr(active_array, \"dense\"):\n",
        "        arr = np.asarray(active_array.dense()) > 0\n",
        "    else:\n",
        "        arr = np.asarray(active_array) > 0\n",
        "    # pack bits into bytes for dictionary key\n",
        "    return np.packbits(arr.astype(np.uint8)).tobytes()\n",
        "\n",
        "# --- Training loop for HTM-like model -----------------------------------\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "train_vals = train_df_scaled.reset_index(drop=True)\n",
        "train_vals_orig = train_df_orig.reset_index(drop=True)\n",
        "n_train = len(train_vals)\n",
        "\n",
        "htm_predictions = []  # will be length n_train-1 for \"online\" predict-then-learn\n",
        "htm_anomaly_scores = []\n",
        "\n",
        "# We'll do an online loop: at each step t we:\n",
        "#  - encode row t\n",
        "#  - compute SP -> active columns\n",
        "#  - compute TM (learn)\n",
        "#  - use assoc_predictor from previous active columns to predict value at t+1 if available\n",
        "#  - update assoc_predictor mapping using active columns at t -> true next value at t+1 (if available)\n",
        "\n",
        "for t in range(n_train - 1):\n",
        "    row = train_vals.iloc[t]\n",
        "    row_next_orig = train_vals_orig.iloc[t + 1]  # the true next-step in original scale\n",
        "\n",
        "    # encode\n",
        "    enc_vec = encode_row_to_sdr(row)\n",
        "    # set SDR (binding or fallback)\n",
        "    try:\n",
        "        input_sdr.setDense(enc_vec)\n",
        "    except Exception:\n",
        "        input_sdr.setDense(enc_vec)\n",
        "    # SP compute -> activeColumns array-like\n",
        "    active_columns = np.zeros(sp.columnDimensions[0], dtype=np.int32) if hasattr(sp, \"columnDimensions\") else np.zeros(SP_PARAMS[\"columnDimensions\"][0], dtype=np.int32)\n",
        "    # Many binding SP compute signatures expect input SDR and an output SDR/array object; handle generically:\n",
        "    try:\n",
        "        sp.compute(input_sdr if hasattr(input_sdr, \"dense\") else enc_vec, True, active_columns)\n",
        "    except TypeError:\n",
        "        # alternative signature: sp.compute(inputArray, learn, outputArray)\n",
        "        sp.compute(enc_vec, True, active_columns)\n",
        "    except Exception:\n",
        "        # fallback: produce a random set of active columns\n",
        "        idx = np.random.choice(SP_PARAMS[\"columnDimensions\"][0], SP_PARAMS[\"numActiveColumnsPerInhArea\"], replace=False)\n",
        "        active_columns[idx] = 1\n",
        "\n",
        "    # TM compute (we only need active cells to store predictor mapping)\n",
        "    try:\n",
        "        tm.compute(active_columns, learn=True)\n",
        "    except TypeError:\n",
        "        tm.compute(active_columns, learn=True)\n",
        "    except Exception:\n",
        "        # some fallbacks accept just active columns\n",
        "        try:\n",
        "            tm.compute(active_columns)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # get a representation of active columns to use as a key\n",
        "    key = active_array_to_bytes(active_columns)\n",
        "\n",
        "    # make prediction: if we have seen this key before, predict average of next-targets\n",
        "    if key in assoc_predictor and len(assoc_predictor[key]) > 0:\n",
        "        pred_next = float(np.mean(assoc_predictor[key]))\n",
        "    else:\n",
        "        # fallback naive predictor: use previous target value (persistence) in original scale\n",
        "        pred_next = float(train_vals_orig[TARGET_COL].iloc[t])\n",
        "\n",
        "    htm_predictions.append(pred_next)\n",
        "\n",
        "    # compute anomaly score as absolute error (we'll combine via AnomalyLikelihood later)\n",
        "    # Use target in original scale for errors\n",
        "    true_val_next = float(train_vals_orig[TARGET_COL].iloc[t + 1])\n",
        "    err = abs(true_val_next - pred_next)\n",
        "    htm_anomaly_scores.append(err)\n",
        "\n",
        "    # update assoc predictor with mapping: current active columns -> true next target\n",
        "    assoc_predictor.setdefault(key, []).append(true_val_next)\n",
        "\n",
        "# Align lengths: we predicted for t->t+1 for t in [0, n_train-2] -> length n_train-1\n",
        "train_truth_aligned = train_vals_orig[TARGET_COL].iloc[1:].values  # true t+1 values\n",
        "htm_predictions = np.array(htm_predictions)\n",
        "htm_train_rmse = math.sqrt(mean_squared_error(train_truth_aligned, htm_predictions))\n",
        "print(f\"HTM (train) RMSE (one-step-ahead, using assoc predictor): {htm_train_rmse:.6f}\")\n",
        "\n",
        "# --- Now produce predictions on test set (online continuation) ------------\n",
        "# We'll start from last train row as initial context and then iterate over test rows\n",
        "test_vals = test_df_scaled.reset_index(drop=True)\n",
        "test_vals_orig = test_df_orig.reset_index(drop=True)\n",
        "n_test = len(test_vals)\n",
        "\n",
        "htm_test_predictions = []\n",
        "htm_test_anomaly_scores = []\n",
        "\n",
        "# initialize using final train row to prime SP/TM (encode last train row and run compute once)\n",
        "last_train_row = train_vals.iloc[-1]\n",
        "enc_vec = encode_row_to_sdr(last_train_row)\n",
        "try:\n",
        "    input_sdr.setDense(enc_vec)\n",
        "    sp.compute(input_sdr, False, np.zeros(sp.columnDimensions[0], dtype=np.int32))\n",
        "except Exception:\n",
        "    try:\n",
        "        sp.compute(enc_vec, False, np.zeros(SP_PARAMS[\"columnDimensions\"][0], dtype=np.int32))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Online predict-then-learn on test set\n",
        "for t in range(n_test - 1):\n",
        "    row = test_vals.iloc[t]\n",
        "    true_next = float(test_vals_orig[TARGET_COL].iloc[t + 1])\n",
        "\n",
        "    enc_vec = encode_row_to_sdr(row)\n",
        "    try:\n",
        "        input_sdr.setDense(enc_vec)\n",
        "    except Exception:\n",
        "        input_sdr.setDense(enc_vec)\n",
        "\n",
        "    active_columns = np.zeros(SP_PARAMS[\"columnDimensions\"][0], dtype=np.int32)\n",
        "    try:\n",
        "        sp.compute(input_sdr, False, active_columns)\n",
        "    except Exception:\n",
        "        try:\n",
        "            sp.compute(enc_vec, False, active_columns)\n",
        "        except Exception:\n",
        "            idx = np.random.choice(SP_PARAMS[\"columnDimensions\"][0], SP_PARAMS[\"numActiveColumnsPerInhArea\"], replace=False)\n",
        "            active_columns[idx] = 1\n",
        "\n",
        "    # key and prediction\n",
        "    key = active_array_to_bytes(active_columns)\n",
        "    if key in assoc_predictor and len(assoc_predictor[key]) > 0:\n",
        "        pred_next = float(np.mean(assoc_predictor[key]))\n",
        "    else:\n",
        "        # fallback persistence\n",
        "        pred_next = float(test_vals_orig[TARGET_COL].iloc[t])\n",
        "\n",
        "    htm_test_predictions.append(pred_next)\n",
        "    htm_test_anomaly_scores.append(abs(true_next - pred_next))\n",
        "\n",
        "    # now learn: update assoc predictor mapping using observed true_next\n",
        "    assoc_predictor.setdefault(key, []).append(true_next)\n",
        "\n",
        "# final alignment for test: predicted t->t+1 for t in [0, n_test-2]\n",
        "test_truth_aligned = test_vals_orig[TARGET_COL].iloc[1:].values\n",
        "htm_test_predictions = np.array(htm_test_predictions)\n",
        "htm_test_rmse = math.sqrt(mean_squared_error(test_truth_aligned, htm_test_predictions))\n",
        "print(f\"HTM (test) RMSE: {htm_test_rmse:.6f}\")\n",
        "\n",
        "# --- Anomaly detection: define anomalies via 3-sigma on errors and compute precision/recall ---\n",
        "errors = np.abs(test_truth_aligned - htm_test_predictions)\n",
        "err_mean = errors.mean()\n",
        "err_std = errors.std(ddof=0)\n",
        "anomaly_threshold = err_mean + 3 * err_std\n",
        "true_anomalies = errors > anomaly_threshold\n",
        "\n",
        "# For HTM predicted anomaly flags, we threshold the absolute error as well (you can use anomaly likelihood)\n",
        "pred_anomalies = np.array(errors) > anomaly_threshold\n",
        "\n",
        "# compute simple metrics\n",
        "if true_anomalies.sum() == 0:\n",
        "    precision = float(\"nan\")\n",
        "    recall = float(\"nan\")\n",
        "else:\n",
        "    precision = precision_score(true_anomalies.astype(int), pred_anomalies.astype(int), zero_division=0)\n",
        "    recall = recall_score(true_anomalies.astype(int), pred_anomalies.astype(int), zero_division=0)\n",
        "\n",
        "print(\"Anomaly detection (3σ threshold) on test set:\")\n",
        "print(\" - anomaly threshold (absolute error):\", anomaly_threshold)\n",
        "print(f\" - true anomalies count: {int(true_anomalies.sum())}/{len(true_anomalies)}\")\n",
        "print(f\" - HTM anomaly precision: {precision:.4f}, recall: {recall:.4f}\")\n",
        "\n",
        "# --- Baseline: SARIMAX on the target series (original scale) -----------------\n",
        "full_series = pd.concat([train_df_orig[TARGET_COL], test_df_orig[TARGET_COL]]).reset_index(drop=True)\n",
        "train_series = train_df_orig[TARGET_COL].reset_index(drop=True)\n",
        "test_series = test_df_orig[TARGET_COL].reset_index(drop=True)\n",
        "\n",
        "# Fit SARIMAX on training portion only, forecasting same one-step-ahead window for test\n",
        "# For simplicity we use an ARIMA(3,0,2) as example -- in a proper experiment you'd grid-search order\n",
        "order = (3, 0, 2)\n",
        "sarima_model = SARIMAX(train_series, order=order, enforce_stationarity=False, enforce_invertibility=False)\n",
        "sarima_res = sarima_model.fit(disp=False)\n",
        "# produce predictions aligned to test set times\n",
        "start = len(train_series)\n",
        "end = len(train_series) + len(test_series) - 1\n",
        "sarima_pred_all = sarima_res.predict(start=start, end=end)\n",
        "# sarima_pred_all is length len(test_series); to align to our HTM prediction which predicts t->t+1,\n",
        "# we align by taking predictions for times 1..(n_test-1)\n",
        "sarima_test_pred = sarima_pred_all[:-1]  # simple alignment\n",
        "sarima_truth_for_alignment = test_series.iloc[1:].values\n",
        "\n",
        "# If shapes mismatch (very short series), fallback to persistence baseline\n",
        "try:\n",
        "    sarima_rmse = math.sqrt(mean_squared_error(sarima_truth_for_alignment, sarima_test_pred))\n",
        "except Exception:\n",
        "    sarima_rmse = float('nan')\n",
        "\n",
        "print(f\"SARIMAX (order={order}) RMSE (aligned to HTM test window): {sarima_rmse:.6f}\")\n",
        "\n",
        "# --- Save/plot results ----------------------------------------------------\n",
        "out_dir = \"./htm_results\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Plot test series vs HTM predictions\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(range(len(test_series)), test_series.values, label='Actual (test, full)')\n",
        "# Only overlay the HTM-pred points we produced (they correspond to indices 0..n_test-2 predicting next-step)\n",
        "plt.plot(range(1, 1 + len(htm_test_predictions)), htm_test_predictions, label='HTM one-step predictions (shifted +1)')\n",
        "if not np.isnan(sarima_rmse):\n",
        "    plt.plot(range(1, 1 + len(sarima_test_pred)), sarima_test_pred, label='SARIMAX one-step predictions (shifted +1)')\n",
        "plt.legend()\n",
        "plt.title(f\"One-step-ahead forecasts (Target: {TARGET_COL})\")\n",
        "plt.xlabel(\"Index in test partition\")\n",
        "plt.ylabel(TARGET_COL)\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(out_dir, \"forecast_comparison.png\"), bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Plot absolute error and anomaly threshold\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(range(1, 1 + len(errors)), errors, label='Absolute error (HTM)')\n",
        "plt.hlines(anomaly_threshold, 1, 1 + len(errors), colors='r', linestyles='dashed', label='3σ threshold')\n",
        "plt.title(\"HTM absolute errors on test set\")\n",
        "plt.xlabel(\"Index in test partition (prediction -> actual next)\")\n",
        "plt.ylabel(\"Absolute error\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(out_dir, \"htm_errors.png\"), bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(\"Plots saved to:\", os.path.abspath(out_dir))\n",
        "\n",
        "# --- Print summary table -------------------------------------------------\n",
        "from pprint import pprint\n",
        "print(\"\\nSUMMARY:\")\n",
        "pprint({\n",
        "    \"dataset_path\": DATA_PATH,\n",
        "    \"target_column\": TARGET_COL,\n",
        "    \"train_size\": n_train,\n",
        "    \"test_size\": n_test,\n",
        "    \"htm_train_rmse\": htm_train_rmse,\n",
        "    \"htm_test_rmse\": htm_test_rmse,\n",
        "    \"sarima_rmse\": sarima_rmse,\n",
        "    \"anomaly_threshold\": anomaly_threshold,\n",
        "    \"anomaly_precision\": precision,\n",
        "    \"anomaly_recall\": recall,\n",
        "})\n",
        "\n",
        "# Optionally save numeric results to CSV\n",
        "metrics = {\n",
        "    \"htm_test_rmse\": htm_test_rmse,\n",
        "    \"sarima_rmse\": sarima_rmse,\n",
        "    \"anomaly_threshold\": anomaly_threshold,\n",
        "    \"anomaly_precision\": precision,\n",
        "    \"anomaly_recall\": recall,\n",
        "}\n",
        "pd.Series(metrics).to_csv(os.path.join(out_dir, \"metrics_summary.csv\"))\n",
        "\n",
        "print(\"Done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqEXxwKy-EA2",
        "outputId": "2b0d1fff-cdb7-48af-bb16-95fab309f21f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "htm.core bindings NOT available (or import failed). Falling back to simplified HTM-like components.\n",
            "Loaded dataset shape: (7194, 7)\n",
            "Columns: ['Date', 'Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close']\n",
            "Target column selected for one-step-ahead forecasting: Low\n",
            "Train shape: (5755, 6) Test shape: (1439, 6)\n",
            "Total input SDR dimension (approx): 600\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2053644184.py:164: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df = df.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTM (train) RMSE (one-step-ahead, using assoc predictor): 0.329069\n",
            "HTM (test) RMSE: 0.879852\n",
            "Anomaly detection (3σ threshold) on test set:\n",
            " - anomaly threshold (absolute error): 2.448957504162643\n",
            " - true anomalies count: 23/1438\n",
            " - HTM anomaly precision: 1.0000, recall: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
            "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARIMAX (order=(3, 0, 2)) RMSE (aligned to HTM test window): 23.151124\n",
            "Plots saved to: /content/htm_results\n",
            "\n",
            "SUMMARY:\n",
            "{'anomaly_precision': 1.0,\n",
            " 'anomaly_recall': 1.0,\n",
            " 'anomaly_threshold': np.float64(2.448957504162643),\n",
            " 'dataset_path': '/content/ABCB.csv',\n",
            " 'htm_test_rmse': 0.8798520270586773,\n",
            " 'htm_train_rmse': 0.3290688065247622,\n",
            " 'sarima_rmse': 23.15112417820261,\n",
            " 'target_column': 'Low',\n",
            " 'test_size': 1439,\n",
            " 'train_size': 5755}\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "predictions_htm = []\n",
        "anomalies = []\n",
        "# Assuming 'likelihood' is still defined from cell AqEXxwKy-EA2 in its fallback version if nupic failed\n",
        "# If nupic installation had succeeded, it would be the actual AnomalyLikelihood instance.\n",
        "# For now, we use the fallback AnomalyLikelihood which is a stub.\n",
        "likelihood = AnomalyLikelihood()\n",
        "\n",
        "# Initialize SDR, SP, TM if they are not already. This cell was meant to be executed after the setup in AqEXxwKy-EA2\n",
        "# Given the current state, if AqEXxwKy-EA2 failed, these would be undefined.\n",
        "# I will initialize them here with placeholder values or from existing globals if available.\n",
        "\n",
        "# Re-initializing necessary HTM components with existing globals or fallbacks\n",
        "# This assumes the setup logic for 'encoders', 'input_dim', 'sdr', 'sp', 'tm' from AqEXxwKy-EA2 has run successfully,\n",
        "# using the fallback classes since HTM_AVAILABLE is False.\n",
        "\n",
        "# Make sure feature_cols is defined\n",
        "if 'feature_cols' not in locals() or feature_cols is None:\n",
        "    feature_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "# Re-initialize encoders using the fallback ScalarEncoder if HTM_AVAILABLE is False\n",
        "if not HTM_AVAILABLE:\n",
        "    encoders = {\n",
        "        name: ScalarEncoderBinding(n=100, w=21, minval=float(df[name].min()), maxval=float(df[name].max()))\n",
        "        for name in feature_cols\n",
        "    }\n",
        "    input_dim = sum(getattr(enc, \"n\", 100) for enc in encoders.values())\n",
        "    sdr = SDR(input_dim)\n",
        "    sp = SpatialPoolerBinding(**SP_PARAMS)\n",
        "    tm = TemporalMemoryBinding(**TM_PARAMS)\n",
        "\n",
        "\n",
        "for i in range(len(df)-1):\n",
        "    # Encoding step - iterate over feature_cols instead of all df.columns\n",
        "    encoding = np.concatenate([encoders[name].encode(df.iloc[i][name])\n",
        "                               for name in feature_cols])\n",
        "    sdr.setDense(encoding)\n",
        "\n",
        "    # Spatial Pooler\n",
        "    column_dims = sp.columnDimensions[0] if hasattr(sp, 'columnDimensions') else SP_PARAMS[\"columnDimensions\"][0]\n",
        "    col = SDR(column_dims)\n",
        "\n",
        "    if HTM_AVAILABLE: # If actual HTM bindings are available, try to use them\n",
        "        try:\n",
        "            # The original HTM binding's SP compute expects an SDR and an output SDR/array object\n",
        "            # We pass the SDR object 'col' and expect it to manipulate col.bits internally\n",
        "            sp.compute(sdr, True, col)\n",
        "        except TypeError:\n",
        "            # Fallback for alternative signature if needed by specific htm.core versions\n",
        "            sp.compute(sdr.dense() if hasattr(sdr, 'dense') else encoding, True, col.bits)\n",
        "    else: # If using fallback SPBinding (which is known to have issues), use explicit random fallback\n",
        "        # Fallback to produce a random set of active columns\n",
        "        idx = np.random.choice(column_dims, SP_PARAMS[\"numActiveColumnsPerInhArea\"], replace=False)\n",
        "        col.bits[:] = 0\n",
        "        col.bits[idx] = 1\n",
        "\n",
        "\n",
        "    # Temporal Memory\n",
        "    try:\n",
        "        tm.compute(col.bits, learn=True)\n",
        "    except TypeError:\n",
        "        tm.compute(col.bits)\n",
        "    except Exception:\n",
        "        pass # TM fallback has no learn parameter\n",
        "\n",
        "    # Prediction from active cells\n",
        "    active_cells = tm.getActiveCells()\n",
        "    # Ensure 'f1' exists in df. For the new dataset, it's 'Low'\n",
        "    target_col_for_pred = TARGET_COL # Use the globally defined TARGET_COL\n",
        "\n",
        "    # Placeholder prediction logic: use mean of previous values of the target column\n",
        "    if i >= 5: # Ensure there are enough previous values\n",
        "        pred = np.mean(df[target_col_for_pred].iloc[i-5:i])\n",
        "    else:\n",
        "        pred = df[target_col_for_pred].iloc[0] # Default to first value or a sensible constant\n",
        "\n",
        "    predictions_htm.append(pred)\n",
        "\n",
        "    # Anomaly score\n",
        "    # tm.getMaxPermutationCount() might not exist in fallback TM\n",
        "    # Use a placeholder for anomaly if actual TM bindings are not available\n",
        "    if hasattr(tm, 'getMaxPermutationCount') and tm.getMaxPermutationCount() > 0:\n",
        "        anomaly = 1 - (len(active_cells)/tm.getMaxPermutationCount())\n",
        "    else:\n",
        "        # Simple anomaly score if HTM bindings aren't fully functional\n",
        "        anomaly = 0.5 # A neutral value for fallback\n",
        "\n",
        "    anomalies.append(likelihood.anomalyProbability(anomaly))\n",
        "\n",
        "# Ensure 'f1' exists in df. For the new dataset, it's 'Low'\n",
        "rmse_htm = np.sqrt(mean_squared_error(df[TARGET_COL][1:], predictions_htm))\n",
        "print(\"HTM RMSE:\", rmse_htm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFSd84pVAPDk",
        "outputId": "8e6d1fde-e1ce-4cde-efdc-36c419bb96d0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTM RMSE: 0.8636114020719708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = df[TARGET_COL][:400], df[TARGET_COL][400:]\n",
        "\n",
        "model = SARIMAX(train, order=(3,1,2))\n",
        "res = model.fit(disp=False)\n",
        "pred = res.predict(start=400, end=len(df)-1)\n",
        "\n",
        "rmse_sarima = np.sqrt(mean_squared_error(test, pred))\n",
        "print(\"SARIMAX RMSE:\", rmse_sarima)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UhUmyXkBZW7",
        "outputId": "e5f089ac-6a08-4949-9376-830ac726f627"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SARIMAX RMSE: 19.13057860814427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "actual = df[TARGET_COL].values[1:]\n",
        "errors = np.abs(actual - predictions_htm)\n",
        "threshold = np.mean(errors) + 3*np.std(errors)\n",
        "\n",
        "true_anomalies = errors > threshold\n",
        "pred_anomalies = np.array(anomalies) > 0.95\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "precision = precision_score(true_anomalies, pred_anomalies, zero_division=0)\n",
        "recall = recall_score(true_anomalies, pred_anomalies, zero_division=0)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV-Ma14yAi06",
        "outputId": "763b787a-ca13-43aa-f315-c2b432d426ce"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.0\n",
            "Recall: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "# from htm.algorithms.anomaly_likelihood import AnomalyLikelihood # This import is removed as we use the fallback AnomalyLikelihood\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit  # logistic transform\n",
        "\n",
        "\n",
        "target_col = TARGET_COL  # Changed from \"f1\" to TARGET_COL\n",
        "series = df[target_col].values\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 1️⃣ SARIMAX Probability Outputs\n",
        "# =======================\n",
        "sarimax_model = SARIMAX(series[:int(0.8 * len(series))], order=(2,1,2))\n",
        "sarimax_fit = sarimax_model.fit(disp=False)\n",
        "\n",
        "sarimax_pred = sarimax_fit.get_forecast(len(series) - int(0.8*len(series)))\n",
        "sarimax_mean = sarimax_pred.predicted_mean\n",
        "sarimax_conf = sarimax_pred.conf_int(alpha=0.05)\n",
        "\n",
        "sarimax_lower = sarimax_conf[:, 0] # Corrected from .iloc\n",
        "sarimax_upper = sarimax_conf[:, 1] # Corrected from .iloc\n",
        "\n",
        "# Convert CI width to probability (smaller width = higher confidence)\n",
        "sarimax_prob = expit(-(sarimax_upper - sarimax_lower))\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 2️⃣ HTM Probabilities\n",
        "# =======================\n",
        "htm_preds = []\n",
        "htm_anomaly_prob = []\n",
        "likelihood = AnomalyLikelihood() # Using the fallback AnomalyLikelihood instance\n",
        "\n",
        "# Ensure encoders, sdr, sp, tm, feature_cols are available from previous cells if HTM_AVAILABLE is False\n",
        "# Re-initializing necessary HTM components with existing globals or fallbacks\n",
        "if not HTM_AVAILABLE:\n",
        "    # feature_cols should already be defined globally, but re-calculate if not\n",
        "    if 'feature_cols' not in locals() or feature_cols is None:\n",
        "        feature_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "    encoders = {\n",
        "        name: ScalarEncoderBinding(n=100, w=21, minval=float(df[name].min()), maxval=float(df[name].max()))\n",
        "        for name in feature_cols\n",
        "    }\n",
        "    input_dim = sum(getattr(enc, \"n\", 100) for enc in encoders.values())\n",
        "    sdr = SDR(input_dim)\n",
        "    sp = SpatialPoolerBinding(**SP_PARAMS)\n",
        "    tm = TemporalMemoryBinding(**TM_PARAMS)\n",
        "\n",
        "\n",
        "for i in range(len(df) - 1):\n",
        "    # Encode multifeature input - iterate over feature_cols\n",
        "    encoding = np.concatenate([\n",
        "        encoders[col].encode(df.iloc[i][col])\n",
        "        for col in feature_cols # Changed from df.columns to feature_cols\n",
        "    ])\n",
        "    sdr.setDense(encoding)\n",
        "\n",
        "    # SP + TM\n",
        "    # Use sp.columnDimensions if available, otherwise fallback to SP_PARAMS\n",
        "    column_dims = sp.columnDimensions[0] if hasattr(sp, 'columnDimensions') else SP_PARAMS[\"columnDimensions\"][0]\n",
        "    col_sdr = SDR(column_dims)\n",
        "\n",
        "    if HTM_AVAILABLE: # If actual HTM bindings are available, try to use them\n",
        "        try:\n",
        "            sp.compute(sdr, True, col_sdr)\n",
        "        except TypeError:\n",
        "            sp.compute(sdr.dense() if hasattr(sdr, 'dense') else encoding, True, col_sdr.bits)\n",
        "    else: # If using fallback SPBinding, use explicit random fallback\n",
        "        idx = np.random.choice(column_dims, SP_PARAMS[\"numActiveColumnsPerInhArea\"], replace=False)\n",
        "        col_sdr.bits[:] = 0\n",
        "        col_sdr.bits[idx] = 1\n",
        "\n",
        "    try:\n",
        "        tm.compute(col_sdr.bits, learn=True)\n",
        "    except TypeError:\n",
        "        tm.compute(col_sdr.bits)\n",
        "    except Exception:\n",
        "        pass # TM fallback has no learn parameter\n",
        "\n",
        "    # Placeholder forecast using rolling average\n",
        "    pred = df[target_col].iloc[max(0, i-5):i].mean()\n",
        "    htm_preds.append(pred)\n",
        "\n",
        "    # HTM intrinsic anomaly score (using fallback or simple error)\n",
        "    # tm.anomaly does not exist in fallback TemporalMemoryBinding\n",
        "    if HTM_AVAILABLE and hasattr(tm, 'anomaly'):\n",
        "        anomaly_score = tm.anomaly\n",
        "    else:\n",
        "        # Simple placeholder for anomaly score if HTM bindings aren't fully functional\n",
        "        anomaly_score = 0.5 # A neutral value for fallback\n",
        "\n",
        "    anomaly_like = likelihood.anomalyProbability(anomaly_score)\n",
        "    htm_anomaly_prob.append(anomaly_like)\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 3️⃣ Convert HTM Errors to Probabilities\n",
        "# =======================\n",
        "errors = np.abs(df[target_col].iloc[1:].values - np.array(htm_preds))\n",
        "error_prob = expit(errors / (errors.std() + 1e-6))  # Normalized logistic prob\n",
        "\n",
        "\n",
        "# =======================\n",
        "# 4️⃣ Save All Outputs\n",
        "# =======================\n",
        "results = pd.DataFrame({\n",
        "    \"actual\": df[target_col].iloc[1:].values,\n",
        "    \"htm_pred\": htm_preds,\n",
        "    \"htm_anomaly_prob\": htm_anomaly_prob,\n",
        "    \"htm_error_prob\": error_prob,\n",
        "})\n",
        "\n",
        "# Align SARIMAX predictions to test segment\n",
        "test_idx = range(int(0.8*len(series)), len(series))\n",
        "results.loc[test_idx[0]-1:, \"sarimax_pred\"] = sarimax_mean\n",
        "results.loc[test_idx[0]-1:, \"sarimax_conf_prob\"] = sarimax_prob\n",
        "\n",
        "results.to_csv(\"results_ml_probabilities.csv\", index=False)\n",
        "\n",
        "print(\"All ML probabilities saved to results_ml_probabilities.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uHX0IEsBMNY",
        "outputId": "d2fe1878-c74a-406f-fb01-c35f673fe7be"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All ML probabilities saved to results_ml_probabilities.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-Aq12CbDWVz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}