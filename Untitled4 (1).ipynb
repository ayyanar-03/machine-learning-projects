{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3mgx_vPHjFM",
        "outputId": "9bad7a45-3976-4371-cb9e-679e425bebe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded data shape: (7194, 7)\n",
            "Columns: ['Date', 'Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close']\n",
            "Using numeric dataframe shape: (7194, 5)\n",
            "htm.core not found. Using lightweight HTM-like fallbacks (script will still run).\n",
            "Input SDR dimension: 500\n",
            "Training size: 5755, Test size: 1439, epochs: 5\n",
            "Epoch 1/5 ...\n",
            "  epoch 1 RMSE (train online): 7.960944\n",
            "Epoch 2/5 ...\n",
            "  epoch 2 RMSE (train online): 7.960367\n",
            "Epoch 3/5 ...\n",
            "  epoch 3 RMSE (train online): 7.960212\n",
            "Epoch 4/5 ...\n",
            "  epoch 4 RMSE (train online): 7.960143\n",
            "Epoch 5/5 ...\n",
            "  epoch 5 RMSE (train online): 7.960103\n",
            "HTM epochs finished in 44.7s\n",
            "HTM test RMSE: 30.340663, MAE: 28.913234\n",
            "SARIMAX test RMSE (aligned): 25.493543, MAE: 21.318417\n",
            "Saved results CSV to: ./results_htm/probabilities_and_predictions.csv\n",
            "Saved forecast overlay to: ./results_htm/forecast_overlay.png\n",
            "Saved anomaly probability timeline to: ./results_htm/anomaly_probability_timeline.png\n",
            "\n",
            "SUMMARY:\n",
            "{\n",
            "  \"htm_test_rmse\": 30.340662818691715,\n",
            "  \"htm_test_mae\": 28.91323431839102,\n",
            "  \"sarima_rmse\": 25.49354258328619,\n",
            "  \"sarima_mae\": 21.318416611321954,\n",
            "  \"htm_anomaly_precision_at_0.9\": NaN,\n",
            "  \"htm_anomaly_recall_at_0.9\": NaN,\n",
            "  \"sarima_anomaly_precision_at_0.9\": NaN,\n",
            "  \"sarima_anomaly_recall_at_0.9\": NaN,\n",
            "  \"results_csv\": \"./results_htm/probabilities_and_predictions.csv\",\n",
            "  \"plots_dir\": \"/content/results_htm\"\n",
            "}\n",
            "All done. Check results in: /content/results_htm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
        "from scipy.special import expit  # logistic transform\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# ---------- Config (user-editable) ----------\n",
        "DATA_PATH = \"/content/ABCB.csv\"   # your uploaded file (use this exact path)\n",
        "RESULTS_DIR = \"./results_htm\"\n",
        "TARGET_COL = \"Close\"               # change if you prefer another column (e.g., \"Adjusted Close\")\n",
        "FEATURES = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]  # at least 5 interdependent features; adjust if needed\n",
        "TEST_FRACTION = 0.2\n",
        "RANDOM_SEED = 42\n",
        "EPOCHS = 5                        # number of epochs (full passes over training data) for HTM\n",
        "SP_COLUMNS = 1024\n",
        "SP_ACTIVE_PER_INH = 40\n",
        "TM_CELLS_PER_COL = 16\n",
        "ENCODER_N = 100\n",
        "ENCODER_W = 21\n",
        "SARIMAX_ORDER = (3,0,2)           # baseline ARIMA order\n",
        "# --------------------------------------------\n",
        "\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# --- Load data ----------------------------------------------------------------\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Loaded data shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Validate feature columns exist\n",
        "for c in FEATURES:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Feature '{c}' not found in dataset. Available cols: {df.columns.tolist()}\")\n",
        "\n",
        "if TARGET_COL not in FEATURES:\n",
        "    FEATURES = [c for c in FEATURES if c != TARGET_COL] + [TARGET_COL]\n",
        "\n",
        "# Keep only numeric features (drop Date)\n",
        "df_numeric = df[FEATURES].copy().apply(pd.to_numeric, errors='coerce')\n",
        "df_numeric = df_numeric.fillna(method='ffill').fillna(method='bfill').fillna(0.0)\n",
        "n = len(df_numeric)\n",
        "print(\"Using numeric dataframe shape:\", df_numeric.shape)\n",
        "\n",
        "# Train/test split (by index/time order)\n",
        "split_idx = int((1 - TEST_FRACTION) * n)\n",
        "train_df = df_numeric.iloc[:split_idx].reset_index(drop=True)\n",
        "test_df = df_numeric.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "# Scale features for encoder stability (we will keep original target for evaluation)\n",
        "scaler = StandardScaler()\n",
        "train_scaled = pd.DataFrame(scaler.fit_transform(train_df), columns=train_df.columns)\n",
        "test_scaled = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n",
        "\n",
        "# --- HTM imports with fallbacks ------------------------------------------------\n",
        "HTM_AVAILABLE = True\n",
        "try:\n",
        "    from htm.bindings.sdr import SDR\n",
        "    from htm.algorithms.anomaly_likelihood import AnomalyLikelihood\n",
        "    # Try actual bindings for SP/TM and encoders (names differ by version)\n",
        "    try:\n",
        "        from htm.bindings.algorithms import SpatialPooler as SpatialPoolerBinding\n",
        "        from htm.bindings.algorithms import TemporalMemory as TemporalMemoryBinding\n",
        "    except Exception:\n",
        "        # some versions\n",
        "        from htm.bindings.algorithms import SP as SpatialPoolerBinding\n",
        "        from htm.bindings.algorithms import TM as TemporalMemoryBinding\n",
        "    try:\n",
        "        from htm.bindings.encoders import ScalarEncoder as ScalarEncoderBinding\n",
        "    except Exception:\n",
        "        from htm.bindings.encoders import ScalarEncoder as ScalarEncoderBinding\n",
        "    print(\"htm.core bindings found; using native HTM implementations.\")\n",
        "except Exception:\n",
        "    HTM_AVAILABLE = False\n",
        "    print(\"htm.core not found. Using lightweight HTM-like fallbacks (script will still run).\")\n",
        "\n",
        "    # Minimal SDR fallback\n",
        "    class SDR:\n",
        "        def __init__(self, size):\n",
        "            self.size = size\n",
        "            self.bits = np.zeros(size, dtype=np.int32)\n",
        "        def setDense(self, arr):\n",
        "            arr = np.asarray(arr)\n",
        "            if arr.size != self.size:\n",
        "                raise ValueError(\"SDR.setDense size mismatch\")\n",
        "            self.bits = (arr != 0).astype(np.int32)\n",
        "        def dense(self):\n",
        "            return self.bits.copy()\n",
        "\n",
        "    # Minimal ScalarEncoder fallback\n",
        "    class ScalarEncoderBinding:\n",
        "        def __init__(self, n=ENCODER_N, w=ENCODER_W, minval=0.0, maxval=1.0):\n",
        "            self.n = int(n)\n",
        "            self.w = int(w)\n",
        "            if self.n <= self.w:\n",
        "                raise ValueError(\"Encoder n must be > w\")\n",
        "            self.minval = float(minval)\n",
        "            self.maxval = float(maxval)\n",
        "            self.buckets = np.linspace(self.minval, self.maxval, self.n - self.w + 1)\n",
        "        def encode(self, value):\n",
        "            v = float(value)\n",
        "            pos = np.searchsorted(self.buckets, v)\n",
        "            out = np.zeros(self.n, dtype=np.int32)\n",
        "            start = max(0, min(self.n - self.w, pos))\n",
        "            out[start:start + self.w] = 1\n",
        "            return out\n",
        "        def getWidth(self):\n",
        "            return self.n\n",
        "\n",
        "    # Very simple SpatialPooler fallback\n",
        "    class SpatialPoolerBinding:\n",
        "        def __init__(self, inputDimensions, columnDimensions, potentialPct=0.85,\n",
        "                     globalInhibition=True, numActiveColumnsPerInhArea=40):\n",
        "            self.inputDimensions = inputDimensions\n",
        "            self.columnDimensions = columnDimensions\n",
        "            self.numActiveColumnsPerInhArea = int(numActiveColumnsPerInhArea)\n",
        "            self.columns = columnDimensions[0]\n",
        "        def compute(self, input_sdr, learn, activeArray):\n",
        "            # naive hashing-based selection of active columns\n",
        "            arr = input_sdr.dense() if hasattr(input_sdr, \"dense\") else np.asarray(input_sdr)\n",
        "            rng = np.random.RandomState(int(arr.sum()) + 1)\n",
        "            idx = rng.choice(self.columns, self.numActiveColumnsPerInhArea, replace=False)\n",
        "            activeArray[:] = 0\n",
        "            activeArray[idx] = 1\n",
        "        def getColumnDimensions(self):\n",
        "            return (self.columns,)\n",
        "\n",
        "    # Very simple TemporalMemory fallback\n",
        "    class TemporalMemoryBinding:\n",
        "        def __init__(self, columnDimensions, cellsPerColumn=TM_CELLS_PER_COL,\n",
        "                     activationThreshold=12, initialPermanence=0.21, connectedPermanence=0.5):\n",
        "            self.columns = columnDimensions[0]\n",
        "            self.cellsPerColumn = cellsPerColumn\n",
        "            self.active_cells = set()\n",
        "        def compute(self, activeColumns, learn=True):\n",
        "            cols = np.where(np.asarray(activeColumns) > 0)[0]\n",
        "            self.active_cells = set([int(c)*self.cellsPerColumn for c in cols])\n",
        "        def getActiveCells(self):\n",
        "            return np.array(sorted(self.active_cells))\n",
        "        def getMaxPermutationCount(self):\n",
        "            return max(1, len(self.active_cells))\n",
        "    # Minimal anomaly likelihood stub\n",
        "    class AnomalyLikelihood:\n",
        "        def __init__(self):\n",
        "            pass\n",
        "        def anomalyLikelihood(self, score, timestamp=None):\n",
        "            # map to 0..1 via logistic normalization\n",
        "            return float(expit(score / (1.0 + abs(score))))\n",
        "\n",
        "# --- Build encoders ----------------------------------------------------------\n",
        "encoders = {}\n",
        "for col in train_scaled.columns:\n",
        "    vmin = float(min(train_scaled[col].min(), test_scaled[col].min()))\n",
        "    vmax = float(max(train_scaled[col].max(), test_scaled[col].max()))\n",
        "    enc = ScalarEncoderBinding(n=ENCODER_N, w=ENCODER_W, minval=vmin, maxval=vmax)\n",
        "    encoders[col] = enc\n",
        "\n",
        "# compute input_dim\n",
        "try:\n",
        "    input_dim = sum(enc.getWidth() for enc in encoders.values())\n",
        "except Exception:\n",
        "    input_dim = sum(getattr(enc, \"n\", ENCODER_N) for enc in encoders.values())\n",
        "\n",
        "print(\"Input SDR dimension:\", input_dim)\n",
        "\n",
        "# instantiate SDR and HTM components\n",
        "sdr = SDR(input_dim)\n",
        "sp = SpatialPoolerBinding(inputDimensions=(input_dim,), columnDimensions=(SP_COLUMNS,),\n",
        "                          potentialPct=0.85, globalInhibition=True, numActiveColumnsPerInhArea=SP_ACTIVE_PER_INH)\n",
        "tm = TemporalMemoryBinding(columnDimensions=(SP_COLUMNS,), cellsPerColumn=TM_CELLS_PER_COL)\n",
        "\n",
        "# Anomaly likelihood helper (if binding available)\n",
        "try:\n",
        "    anomaly_lik = AnomalyLikelihood()\n",
        "except Exception:\n",
        "    anomaly_lik = AnomalyLikelihood()\n",
        "\n",
        "# associative predictor: map active columns bitmask -> list(next_target_values)\n",
        "assoc_predictor = {}\n",
        "\n",
        "def encode_row_to_sdr(row_scaled):\n",
        "    parts = []\n",
        "    for name in train_scaled.columns:\n",
        "        val = float(row_scaled[name])\n",
        "        parts.append(np.asarray(encoders[name].encode(val), dtype=np.int32))\n",
        "    arr = np.concatenate(parts).astype(np.int32)\n",
        "    if arr.size != input_dim:\n",
        "        if arr.size < input_dim:\n",
        "            pad = np.zeros(input_dim - arr.size, dtype=np.int32)\n",
        "            arr = np.concatenate([arr, pad])\n",
        "        else:\n",
        "            arr = arr[:input_dim]\n",
        "    return arr\n",
        "\n",
        "def active_cols_key(active_array):\n",
        "    arr = np.asarray(active_array)\n",
        "    # pack bits to bytes to make a hashable key\n",
        "    packed = np.packbits((arr > 0).astype(np.uint8))\n",
        "    return packed.tobytes()\n",
        "\n",
        "# --- HTM training loop with epochs ------------------------------------------\n",
        "train_len = len(train_scaled)\n",
        "print(f\"Training size: {train_len}, Test size: {len(test_scaled)}, epochs: {EPOCHS}\")\n",
        "\n",
        "htm_train_preds = None\n",
        "start_time = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} ...\")\n",
        "    epoch_preds = []\n",
        "    for t in range(train_len - 1):\n",
        "        row = train_scaled.iloc[t]\n",
        "        next_true_orig = train_df[TARGET_COL].iloc[t + 1]  # original scale next-step\n",
        "        enc_vec = encode_row_to_sdr(row)\n",
        "        sdr.setDense(enc_vec)\n",
        "\n",
        "        # compute SP active columns (output array)\n",
        "        active_cols = np.zeros(SP_COLUMNS, dtype=np.int32)\n",
        "        try:\n",
        "            sp.compute(sdr, True, active_cols)\n",
        "        except Exception:\n",
        "            sp.compute(enc_vec, True, active_cols)\n",
        "\n",
        "        # compute TM\n",
        "        try:\n",
        "            tm.compute(active_cols, learn=True)\n",
        "        except Exception:\n",
        "            tm.compute(active_cols)\n",
        "\n",
        "        # predictor via associative map\n",
        "        key = active_cols_key(active_cols)\n",
        "        if key in assoc_predictor and len(assoc_predictor[key]) > 0:\n",
        "            pred = float(np.mean(assoc_predictor[key]))\n",
        "        else:\n",
        "            # fallback: persistence on the target (previous value)\n",
        "            pred = float(train_df[TARGET_COL].iloc[t])\n",
        "\n",
        "        epoch_preds.append(pred)\n",
        "        # update associative predictor with the observed next target (original scale)\n",
        "        assoc_predictor.setdefault(key, []).append(float(next_true_orig))\n",
        "\n",
        "    # compute epoch RMSE on training portion (aligned)\n",
        "    train_truth_aligned = train_df[TARGET_COL].iloc[1:len(epoch_preds)+1].values\n",
        "    epoch_rmse = math.sqrt(mean_squared_error(train_truth_aligned, epoch_preds))\n",
        "    print(f\"  epoch {epoch+1} RMSE (train online): {epoch_rmse:.6f}\")\n",
        "\n",
        "    # keep last epoch preds for reporting\n",
        "    htm_train_preds = epoch_preds\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print(f\"HTM epochs finished in {train_time:.1f}s\")\n",
        "\n",
        "# --- Produce HTM predictions on test set (online continutation) --------------\n",
        "# Prime with last train row\n",
        "last_train_row = train_scaled.iloc[-1]\n",
        "sdr.setDense(encode_row_to_sdr(last_train_row))\n",
        "_dummy_active = np.zeros(SP_COLUMNS, dtype=np.int32)\n",
        "try:\n",
        "    sp.compute(sdr, False, _dummy_active)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "test_len = len(test_scaled)\n",
        "htm_test_preds = []\n",
        "htm_htm_anomaly_scores = []\n",
        "htm_anomaly_likelihoods = []\n",
        "\n",
        "for t in range(test_len - 1):\n",
        "    row = test_scaled.iloc[t]\n",
        "    true_next = float(test_df[TARGET_COL].iloc[t + 1])\n",
        "\n",
        "    enc_vec = encode_row_to_sdr(row)\n",
        "    sdr.setDense(enc_vec)\n",
        "\n",
        "    active_cols = np.zeros(SP_COLUMNS, dtype=np.int32)\n",
        "    try:\n",
        "        sp.compute(sdr, False, active_cols)\n",
        "    except Exception:\n",
        "        sp.compute(enc_vec, False, active_cols)\n",
        "\n",
        "    try:\n",
        "        tm.compute(active_cols, learn=False)\n",
        "    except Exception:\n",
        "        tm.compute(active_cols)\n",
        "\n",
        "    key = active_cols_key(active_cols)\n",
        "    if key in assoc_predictor and len(assoc_predictor[key]) > 0:\n",
        "        pred_next = float(np.mean(assoc_predictor[key]))\n",
        "    else:\n",
        "        pred_next = float(test_df[TARGET_COL].iloc[t])  # persistence fallback\n",
        "\n",
        "    htm_test_preds.append(pred_next)\n",
        "\n",
        "    # compute HTM intrinsic anomaly score (fallback if TM doesn't provide numeric score)\n",
        "    try:\n",
        "        active_cells = tm.getActiveCells()\n",
        "        anomaly_score = 1.0 - (len(active_cells) / max(1.0, tm.getMaxPermutationCount()))\n",
        "    except Exception:\n",
        "        # fallback to error-based raw score\n",
        "        anomaly_score = abs(true_next - pred_next)\n",
        "\n",
        "    # anomaly likelihood\n",
        "    try:\n",
        "        a_like = anomaly_lik.anomalyLikelihood(anomaly_score)\n",
        "    except Exception:\n",
        "        try:\n",
        "            a_like = anomaly_lik.anomalyLikelihood(anomaly_score, timestamp=None)\n",
        "        except Exception:\n",
        "            a_like = float(expit(-anomaly_score))  # fallback mapping\n",
        "\n",
        "    htm_htm_anomaly_scores.append(anomaly_score)\n",
        "    htm_anomaly_likelihoods.append(a_like)\n",
        "\n",
        "# Align arrays for metric computation (we have predictions t->t+1 for indices 0..test_len-2)\n",
        "test_truth_aligned = test_df[TARGET_COL].iloc[1:len(htm_test_preds)+1].values\n",
        "htm_test_rmse = math.sqrt(mean_squared_error(test_truth_aligned, htm_test_preds))\n",
        "htm_test_mae = np.mean(np.abs(test_truth_aligned - np.array(htm_test_preds)))\n",
        "print(f\"HTM test RMSE: {htm_test_rmse:.6f}, MAE: {htm_test_mae:.6f}\")\n",
        "\n",
        "# --- SARIMAX baseline (fit on train original-scale target series) -----------\n",
        "train_series = train_df[TARGET_COL].reset_index(drop=True)\n",
        "test_series = test_df[TARGET_COL].reset_index(drop=True)\n",
        "\n",
        "sarima_model = SARIMAX(train_series, order=SARIMAX_ORDER, enforce_stationarity=False, enforce_invertibility=False)\n",
        "sarima_res = sarima_model.fit(disp=False)\n",
        "# Forecast for full test range\n",
        "sarima_forecast_res = sarima_res.get_forecast(steps=len(test_series))\n",
        "sarima_mean = sarima_forecast_res.predicted_mean.values  # length = len(test_series)\n",
        "sarima_ci = sarima_forecast_res.conf_int(alpha=0.05)    # DataFrame with lower/upper columns\n",
        "\n",
        "# Align SARIMAX one-step alignment as we used for HTM (drop last pred)\n",
        "sarima_test_preds_aligned = sarima_mean[:-1] if len(sarima_mean) > 1 else sarima_mean\n",
        "sarima_truth_aligned = test_series.iloc[1:].values if len(test_series) > 1 else test_series.values\n",
        "\n",
        "try:\n",
        "    sarima_rmse = math.sqrt(mean_squared_error(sarima_truth_aligned, sarima_test_preds_aligned))\n",
        "    sarima_mae = np.mean(np.abs(sarima_truth_aligned - sarima_test_preds_aligned))\n",
        "except Exception:\n",
        "    sarima_rmse = float('nan')\n",
        "    sarima_mae = float('nan')\n",
        "\n",
        "print(f\"SARIMAX test RMSE (aligned): {sarima_rmse:.6f}, MAE: {sarima_mae:.6f}\")\n",
        "\n",
        "# Convert SARIMAX CI width to confidence-probability (smaller CI -> higher confidence)\n",
        "sarima_ci_width = (sarima_ci.iloc[:,1] - sarima_ci.iloc[:,0]).values\n",
        "# Align sarima_ci_width to aligned preds (drop last)\n",
        "if len(sarima_ci_width) > 1:\n",
        "    sarima_ci_width_aligned = sarima_ci_width[:-1]\n",
        "else:\n",
        "    sarima_ci_width_aligned = sarima_ci_width\n",
        "# map width -> [0,1] probability via negative logistic\n",
        "sarima_conf_prob = expit(- (sarima_ci_width_aligned / (np.std(sarima_ci_width_aligned) + 1e-8)))\n",
        "\n",
        "# --- Error-based anomaly probabilities (for HTM & SARIMAX) -------------------\n",
        "htm_errors = np.abs(test_truth_aligned - np.array(htm_test_preds))\n",
        "# logistic probability where larger error -> higher anomaly probability\n",
        "htm_error_prob = expit((htm_errors - np.mean(htm_errors)) / (np.std(htm_errors) + 1e-8))\n",
        "\n",
        "if len(sarima_test_preds_aligned) == len(sarima_truth_aligned):\n",
        "    sarima_errors = np.abs(sarima_truth_aligned - sarima_test_preds_aligned)\n",
        "    sarima_error_prob = expit((sarima_errors - np.mean(sarima_errors)) / (np.std(sarima_errors) + 1e-8))\n",
        "else:\n",
        "    sarima_error_prob = np.array([np.nan]*len(sarima_conf_prob))\n",
        "\n",
        "# --- Anomaly detection metrics using a threshold on error_prob (example 0.9) ---\n",
        "threshold = 0.9\n",
        "htm_pred_anoms = (htm_error_prob > threshold).astype(int)\n",
        "# define true anomalies as points where error > mean + 3*std (3-sigma rule)\n",
        "true_anoms = (htm_errors > (np.mean(htm_errors) + 3 * np.std(htm_errors))).astype(int)\n",
        "\n",
        "if true_anoms.sum() > 0:\n",
        "    htm_prec = precision_score(true_anoms, htm_pred_anoms, zero_division=0)\n",
        "    htm_rec = recall_score(true_anoms, htm_pred_anoms, zero_division=0)\n",
        "else:\n",
        "    htm_prec = float('nan')\n",
        "    htm_rec = float('nan')\n",
        "\n",
        "# For sarima, align shapes before computing\n",
        "if len(sarima_error_prob) == len(true_anoms):\n",
        "    sarima_pred_anoms = (sarima_error_prob > threshold).astype(int)\n",
        "    if true_anoms.sum() > 0:\n",
        "        sarima_prec = precision_score(true_anoms, sarima_pred_anoms, zero_division=0)\n",
        "        sarima_rec = recall_score(true_anoms, sarima_pred_anoms, zero_division=0)\n",
        "    else:\n",
        "        sarima_prec = float('nan')\n",
        "        sarima_rec = float('nan')\n",
        "else:\n",
        "    sarima_prec = sarima_rec = float('nan')\n",
        "\n",
        "# --- Save results to CSV ----------------------------------------------------\n",
        "results_df = pd.DataFrame({\n",
        "    \"actual_next\": test_truth_aligned,\n",
        "    \"htm_pred\": np.array(htm_test_preds),\n",
        "    \"htm_error_abs\": htm_errors,\n",
        "    \"htm_error_prob\": htm_error_prob,\n",
        "    \"htm_anomaly_likelihood\": np.array(htm_anomaly_likelihoods),\n",
        "})\n",
        "\n",
        "# add sarima columns when available/aligned\n",
        "if len(sarima_test_preds_aligned) == len(sarima_truth_aligned):\n",
        "    results_df[\"sarima_pred\"] = sarima_test_preds_aligned\n",
        "    results_df[\"sarima_error_prob\"] = sarima_error_prob\n",
        "    results_df[\"sarima_conf_prob\"] = sarima_conf_prob\n",
        "else:\n",
        "    # pad with NaNs if mismatch\n",
        "    results_df[\"sarima_pred\"] = np.nan\n",
        "    results_df[\"sarima_error_prob\"] = np.nan\n",
        "    results_df[\"sarima_conf_prob\"] = np.nan\n",
        "\n",
        "csv_path = os.path.join(RESULTS_DIR, \"probabilities_and_predictions.csv\")\n",
        "results_df.to_csv(csv_path, index=False)\n",
        "print(\"Saved results CSV to:\", csv_path)\n",
        "\n",
        "# --- Save model snapshot (associative predictor + hyperparams) -------------\n",
        "snapshot = {\n",
        "    \"assoc_predictor_sample_count\": len(assoc_predictor),\n",
        "    \"assoc_predictor_sample_keys\": [k.hex() for k in list(assoc_predictor.keys())[:10]], # Convert bytes to hex string\n",
        "    \"hyperparameters\": {\n",
        "        \"SP_COLUMNS\": SP_COLUMNS,\n",
        "        \"SP_ACTIVE_PER_INH\": SP_ACTIVE_PER_INH,\n",
        "        \"TM_CELLS_PER_COL\": TM_CELLS_PER_COL,\n",
        "        \"EPOCHS\": EPOCHS,\n",
        "        \"ENCODER_N\": ENCODER_N, \"ENCODER_W\": ENCODER_W\n",
        "    },\n",
        "    \"train_time_seconds\": train_time,\n",
        "    \"htm_train_rmse_last_epoch\": epoch_rmse\n",
        "}\n",
        "with open(os.path.join(RESULTS_DIR, \"htm_snapshot.pkl\"), \"wb\") as f:\n",
        "    pickle.dump(snapshot, f)\n",
        "with open(os.path.join(RESULTS_DIR, \"htm_snapshot_summary.json\"), \"w\") as f:\n",
        "    json.dump(snapshot, f, indent=2)\n",
        "\n",
        "# --- Plots: forecast overlay and anomaly probability timeline ----------------\n",
        "plt.figure(figsize=(12,5))\n",
        "# actual full test series\n",
        "plt.plot(range(len(test_series)), test_series.values, label=\"Actual (test full)\")\n",
        "# HTM preds shifted +1 because preds are for next step\n",
        "plt.plot(range(1, 1 + len(htm_test_preds)), htm_test_preds, label=\"HTM one-step preds (shifted +1)\")\n",
        "# SARIMAX overlay if available\n",
        "if len(sarima_test_preds_aligned) == len(sarima_truth_aligned):\n",
        "    plt.plot(range(1, 1 + len(sarima_test_preds_aligned)), sarima_test_preds_aligned, label=\"SARIMAX one-step preds (shifted +1)\")\n",
        "plt.xlabel(\"Index in test partition\")\n",
        "plt.ylabel(TARGET_COL)\n",
        "plt.title(\"One-step-ahead Forecasts (Actual vs HTM vs SARIMAX)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"forecast_overlay.png\"), bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved forecast overlay to:\", os.path.join(RESULTS_DIR, \"forecast_overlay.png\"))\n",
        "\n",
        "# Anomaly probability timeline\n",
        "plt.figure(figsize=(12,4))\n",
        "x = range(1, 1 + len(htm_error_prob))\n",
        "plt.plot(x, htm_anomaly_likelihoods, label=\"HTM anomaly likelihood\")\n",
        "plt.plot(x, htm_error_prob, label=\"HTM error-based prob\")\n",
        "if len(sarima_conf_prob) == len(x):\n",
        "    plt.plot(x, sarima_conf_prob, label=\"SARIMAX CI->prob (aligned)\")\n",
        "if len(sarima_error_prob) == len(x):\n",
        "    plt.plot(x, sarima_error_prob, label=\"SARIMAX error-based prob (aligned)\")\n",
        "plt.xlabel(\"Index in test partition (prediction -> actual next)\")\n",
        "plt.ylabel(\"Probability\")\n",
        "plt.title(\"Anomaly / Confidence Probabilities over Test Window\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(RESULTS_DIR, \"anomaly_probability_timeline.png\"), bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved anomaly probability timeline to:\", os.path.join(RESULTS_DIR, \"anomaly_probability_timeline.png\"))\n",
        "\n",
        "# --- Summary outputs to stdout ------------------------------------------------\n",
        "summary = {\n",
        "    \"htm_test_rmse\": htm_test_rmse,\n",
        "    \"htm_test_mae\": htm_test_mae,\n",
        "    \"sarima_rmse\": sarima_rmse,\n",
        "    \"sarima_mae\": sarima_mae,\n",
        "    \"htm_anomaly_precision_at_0.9\": htm_prec,\n",
        "    \"htm_anomaly_recall_at_0.9\": htm_rec,\n",
        "    \"sarima_anomaly_precision_at_0.9\": sarima_prec,\n",
        "    \"sarima_anomaly_recall_at_0.9\": sarima_rec,\n",
        "    \"results_csv\": csv_path,\n",
        "    \"plots_dir\": os.path.abspath(RESULTS_DIR)\n",
        "}\n",
        "\n",
        "print(\"\\nSUMMARY:\")\n",
        "print(json.dumps(summary, indent=2))\n",
        "\n",
        "# Save summary json\n",
        "with open(os.path.join(RESULTS_DIR, \"summary_metrics.json\"), \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"All done. Check results in:\", os.path.abspath(RESULTS_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQzOvQlxHvOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}